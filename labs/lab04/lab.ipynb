{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 ‚Äì Hypothesis and Permutation Testing\n",
    "\n",
    "## DSC 80, Fall 2024\n",
    "\n",
    "### Due Date: Monday, October 25th at 11:59PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Welcome to the fourth DSC 80 lab this quarter!\n",
    "\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and Markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding will be done in an accompanying `lab.py` file that is imported into the current notebook, and **you will only submit that `lab.py` file**, not this notebook!\n",
    "\n",
    "Some additional guidelines:\n",
    "- **Unlike in DSC 10, labs will have both public tests and hidden tests.** The bulk of your grade will come from your scores on hidden tests, which you will only see on Gradescope after the assignment deadline.\n",
    "- **Do not change the function names in the `lab.py` file!** The functions in the `lab.py` file are how your assignment is graded, and they are graded by their name. If you changed something you weren't supposed to, you can find the original code in the [course GitHub repository](https://github.com/dsc-courses/dsc80-2024-fa).\n",
    "- Notebooks are nice for testing and experimenting with different implementations before designing your function in your `lab.py` file. You can write code here, but make sure that all of your real work is in the `lab.py` file, since that's all you're submitting.\n",
    "- You are encouraged to write your own additional helper functions to solve the lab, as long as they also end up in `lab.py`.\n",
    "\n",
    "**To ensure that all of the work you want to submit is in `lab.py`, we've included a script named `lab-validation.py` in the lab folder. You shouldn't edit it, but instead, you should call it from the command line (e.g. the Terminal) to test your work.** More details on its usage are given at the bottom of this notebook.\n",
    "\n",
    "**Importing code from `lab.py`**:\n",
    "\n",
    "* Below, we import the `.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" markdown=\"1\">\n",
    "\n",
    "**You cannot use `for`-loops in Part 1 (Time Series Data) or Part 2 (Hypothesis Testing), but you can use them in Part 3 (Permutation Testing).**\n",
    "\n",
    "</div>\n",
    "\n",
    "## Part 1: Time Series Data\n",
    "\n",
    "Imagine that you own an online store and you'd like to monitor the visits to your site. You've collected information about different login dates and times for different users and stored it in `data/login_table.csv`. Some users are unique, while some visited your store multiple times.\n",
    "\n",
    "Answer the questions below to better understand the login patterns of your users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 ‚Äì Prime Time ‚è∞\n",
    "\n",
    "Complete the implementation of the function `prime_time_logins`, which takes in a DataFrame like `login` and outputs a DataFrame indexed by `'Login Id'`, counting the number of prime-time logins for each user ‚Äì that is, the number of logins that were between 4PM (inclusive) and 8PM (exclusive) for each user. The DataFrame should have just one column, named `'Time'`.\n",
    "\n",
    "For example, if a user logs in at 5PM on Day 1, at 1PM on Day 2, at 6PM on Day 2, and at 7PM on Day 2, then their total number of prime-time logins is 3. Note that the values in your returned DataFrame should only include counts, not timestamp objects.\n",
    "\n",
    "***Note***: You do not need to use Python's `datetime` module ‚Äì instead, use the built-in `pandas` methods for working with times that we introduced in [Lecture 5](https://dsc80.com/resources/lectures/lec05/lec05.html#Creating-timestamps) (though you may need to do a bit more research to fully answer the question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prime_time_logins(login_df):\n",
    "#     '''\n",
    "    \n",
    "#     after set index to login id?\n",
    "#     do a broadcast comparsion of times in a range\n",
    "#     4 up 8pm\n",
    "\n",
    "#     date_format = '%Y-%m-%d'\n",
    "#     pd.to_datetime(insp['date'], format=date_format)\n",
    "    \n",
    "#     '''\n",
    "#     # Set 'Login Id' as the index\n",
    "#     login_df = login_df.set_index('Login Id')\n",
    "\n",
    "\n",
    "#     login_df['Time'] = pd.to_datetime(login_df['Time'], format='%Y-%m-%d %H:%M:%S')\n",
    "#     # Extract the hour component from the datetime values\n",
    "#         # NOTICE : this won't work without the explicit date_format made above\n",
    "#     hours = login_df['Time'].dt.hour\n",
    "    \n",
    "#     # Create a mask for prime-time logins (4PM to 8PM)\n",
    "#     prime_time_mask = (hours >= 16) & (hours < 20)\n",
    "    \n",
    "#     # Count prime-time logins for each user\n",
    "#     prime_time_count = login_df[prime_time_mask].groupby(level=0).size().rename('Time')\n",
    "    \n",
    "#     return prime_time_count\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prime_time_logins(login_df):\n",
    "    '''\n",
    "    Takes in a DataFrame with 'Login Id' and 'Time' columns,\n",
    "    and returns a DataFrame with the count of prime-time logins for each user.\n",
    "    Prime-time is defined as between 4PM (inclusive) and 8PM (exclusive).\n",
    "    '''\n",
    "    # Make a copy of the DataFrame to avoid modifying the original\n",
    "    login_df = login_df.copy()\n",
    "\n",
    "    # Convert 'Time' column to datetime\n",
    "    login_df['Time'] = pd.to_datetime(login_df['Time'], format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Extract the hour component from the datetime values\n",
    "    hours = login_df['Time'].dt.hour\n",
    "    \n",
    "    # Create a mask for prime-time logins (4PM to 8PM)\n",
    "    prime_time_mask = (hours >= 16) & (hours < 20)\n",
    "    \n",
    "    # Count prime-time logins for each user\n",
    "    prime_time_count = login_df[prime_time_mask].groupby('Login Id').size().to_frame('Time')\n",
    "    \n",
    "    # Ensure all users are included, even those with zero prime-time logins\n",
    "    all_users = login_df['Login Id'].unique()\n",
    "    prime_time_count = prime_time_count.reindex(all_users, fill_value=0)\n",
    "    \n",
    "    return prime_time_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "fp = Path('data') / 'login_table.csv'\n",
    "login = pd.read_csv(fp)\n",
    "q1_result = prime_time_logins(login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time    4\n",
      "Name: 457, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(q1_result.loc[457])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Login Id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>433 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Time\n",
       "Login Id      \n",
       "466         38\n",
       "458         52\n",
       "592         38\n",
       "393         12\n",
       "636          0\n",
       "...        ...\n",
       "1302         2\n",
       "1304         0\n",
       "1305         1\n",
       "1306         1\n",
       "1307         0\n",
       "\n",
       "[433 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prime_time_logins(login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q1 results: All test cases passed!"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 ‚Äì Return Users üîÅ\n",
    "\n",
    "As a site owner, you would like to find your most enthusiastic users ‚Äì the ones who return to your site most frequently. You've noticed that there are users who have several logins and users who logged in only once. You are interested in finding the number of logins *per day* for each user ‚Äì that is, **their total number of logins divided by the total number of days they've been on your site**.\n",
    "\n",
    "Complete the implementation of the function `count_frequency`, which takes in a DataFrame like `login` and outputs a Series containing the number of logins per day for each user. Your Series should have `'Login Id'`s in its index, and the frequencies as its values. The order of users in the index is arbitrary.\n",
    "\n",
    "To do this, you should assume today is January 31st, 2024 and use 11:59:00PM as the current time. The first login date of a user is the first day of their membership on the site, and you may assume they are still a member today. For simplicity, you only need to count full days that a user has been a member till the end of today. For example, if a user's first login was 12 days and 5 hours ago, you can say that they have been a user for 12 days. \n",
    "\n",
    "***Hint***: Can you write a custom aggregator that allows you to do this with just one call of the `groupby` method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_frequency(login_df):\n",
    "#     '''\n",
    "#     use date time like early same format\n",
    "#     make a conversion of the date time to quantity of days\n",
    "    \n",
    "    \n",
    "#     '''\n",
    "#     # Make a copy of the DataFrame to avoid modifying the original\n",
    "#     login_df = login_df.copy()\n",
    "    \n",
    "#     # Convert 'Time' column to datetime if it's not already\n",
    "#     login_df['Time'] = pd.to_datetime(login_df['Time'])\n",
    "    \n",
    "#     # Set the end date (January 31st, 2024 at 11:59:00 PM)\n",
    "#     end_date = pd.Timestamp('2024-01-31 23:59:00')\n",
    "    \n",
    "#     # Define a custom aggregation function\n",
    "#     def login_frequency(group):\n",
    "#         first_login = group.min()\n",
    "#         total_logins = group.count()\n",
    "#         days_as_member = (end_date - first_login).days + 1  # +1 to include the last day\n",
    "#         return total_logins / days_as_member\n",
    "    \n",
    "#     # Group by 'Login Id' and apply the custom aggregation\n",
    "#     result = login_df.groupby('Login Id')['Time'].agg(login_frequency)\n",
    "#     return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def count_frequency(login_df):\n",
    "#     \"\"\"\n",
    "#     Calculate the number of logins per day for each user.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     login_df : pandas.DataFrame\n",
    "#         DataFrame containing 'Login Id' and 'Time' columns\n",
    "        \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     pandas.Series\n",
    "#         Series with Login Ids as index and login frequency (logins per day) as values\n",
    "#     \"\"\"\n",
    "#     # Make a copy of the DataFrame to avoid modifying the original\n",
    "#     df = login_df.copy()\n",
    "    \n",
    "#     # Convert 'Time' column to datetime if it's not already\n",
    "#     df['Time'] = pd.to_datetime(df['Time'])\n",
    "    \n",
    "#     # Set reference end date (January 31st, 2024 at 11:59:00 PM)\n",
    "#     end_date = pd.Timestamp('2024-01-31 23:59:00')\n",
    "    \n",
    "#     def calculate_frequency(group):\n",
    "#         \"\"\"\n",
    "#         Custom aggregator function to calculate login frequency\n",
    "#         \"\"\"\n",
    "#         # Get first login date and total number of logins\n",
    "#         first_login = group.min()\n",
    "#         total_logins = len(group)\n",
    "        \n",
    "#         # Calculate total days (including the first and last day)\n",
    "#         days_as_member = (end_date - first_login).days + 1\n",
    "        \n",
    "#         # Calculate and return frequency\n",
    "#         return total_logins / days_as_member\n",
    "    \n",
    "#     # Group by Login Id and apply the custom aggregator\n",
    "#     result = df.groupby('Login Id')['Time'].agg(calculate_frequency)\n",
    "    \n",
    "#     return result\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frequency(login_df):\n",
    "    # Make a copy of the DataFrame to avoid modifying the original\n",
    "    login_df = login_df.copy()\n",
    "    \n",
    "    # Convert 'Time' column to datetime\n",
    "    login_df['Time'] = pd.to_datetime(login_df['Time'])\n",
    "    \n",
    "    # Set the end date to January 31st, 2024, at 11:59 PM\n",
    "    end_date = pd.Timestamp('2024-01-31 23:59:00')\n",
    "    \n",
    "    # Define a custom aggregation function to calculate login frequency\n",
    "    def login_frequency(group):\n",
    "        first_login = group.min()  # First login date per user\n",
    "        total_logins = group.size  # Use 'size' as a property, not a method\n",
    "        days_as_member = (end_date - first_login).days  # Days as member (inclusive)\n",
    "        if days_as_member == 0:\n",
    "            return total_logins  # Handle users with logins only on one day\n",
    "        return total_logins / (days_as_member)  # +1 to include the last day\n",
    "    \n",
    "    # Group by 'Login Id' and apply the custom aggregation\n",
    "    result = login_df.groupby('Login Id')['Time'].agg(login_frequency)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "fp = Path('data') / 'login_table.csv'\n",
    "login = pd.read_csv(fp)\n",
    "q2_result = count_frequency(login)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.24250681198910082)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q2_result.loc[466]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q2</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q2 results: All test cases passed!"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" markdown=\"1\">\n",
    "\n",
    "**You cannot use `for`-loops in Part 1 (Time Series Data) or Part 2 (Hypothesis Testing), but you can use them in Part 3 (Permutation Testing).**\n",
    "\n",
    "</div>\n",
    "\n",
    "## Part 2: Hypothesis Testing\n",
    "\n",
    "In this section, you'll recall the terms and structure of hypothesis testing from DSC 10.\n",
    "\n",
    "The first step is always to define what you're looking at, create your hypotheses, and set a level of significance (i.e. a p-value cutoff). Once you've done that, you can find a p-value.\n",
    "\n",
    "If all of these words are foreign, look at the [Lecture 6](https://dsc80.com/resources/lectures/lec06/lec06.html) notebook and the readings, and don't forget to think about the real-world meaning of these terms!  The following example describes a real-world scenario, which should help keep it easy to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 ‚Äì Baking Sale üßÅ\n",
    "\n",
    "At UC San Diego, students are looking to buy treats at a bake sale on Library Walk. There is a pop-up bake stand selling cookies and cupcakes to students. Last Saturday, this stand sold 250 cookies to UCSD students. After eating the cookies, 15 students complained that their cookies were burnt, leaving a bitter taste in their mouths. In response to the student dissatisfaction, the stand claims that 96% of their cookies are baked perfectly without any burning. You think this seems unlikely and decide to investigate.\n",
    "\n",
    "First, select a significance level for your investigation. You don't need to turn this in anywhere. Then, complete the implementation of the following two functions.\n",
    "\n",
    "#### `cookies_null_hypothesis`\n",
    "\n",
    "Complete the implementation of the function `cookies_null_hypothesis`, takes in no arguments and returns your answer(s) to the following question **as a list**.\n",
    "\n",
    "What are reasonable choices for the **null hypothesis** for your investigation? Select all that apply.\n",
    "1. The stand sells cookies that are approximately 4% burnt. \n",
    "2. The stand sells cookies that are 96% perfectly baked.\n",
    "3. The stand sells cookies that are less than 96% perfectly baked. \n",
    "4. The stand sells cookies that are at least 4% burnt.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `cookies_p_value`\n",
    "\n",
    "Complete the implementation of the function `cookies_p_value`, which takes in an integer `N` and returns the estimated p-value of your investigation upon simulating the null hypothesis `N` times. (The p-value is an estimate of the true theoretical p-value of your test since it relies on simulation.)\n",
    "\n",
    "***Note***: Plot the null distribution and your observed statistic to check your work. (If you decide to plot, you may have to run `import matplotlib.pyplot as plt` or `import plotly.express as px`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>why was this code not working and expecting a specific set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cookies_null_hypothesis():\n",
    "#     \"\"\"\n",
    "#     Returns a list of reasonable choices for the null hypothesis.\n",
    "#     \"\"\"\n",
    "#     return [\n",
    "#         \"The stand sells cookies that are approximately 4% burnt.\",\n",
    "#         \"The stand sells cookies that are 96% perfectly baked.\",\n",
    "#         \"The stand sells cookies that are at least 96% perfectly baked.\"\n",
    "#     ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def cookies_p_value(N_simulations):\n",
    "#     \"\"\"\n",
    "#         GOAL get the tests onesided and two sided correctl with 96% of cookies are NOT burnt\n",
    "#         claim\n",
    "    \n",
    "#     \"\"\"\n",
    "#     # Define the parameters of the null hypothesis\n",
    "#     n_cookies = 250\n",
    "#     p_burnt = 0.04\n",
    "    \n",
    "#     # Simulate the null hypothesis N times\n",
    "#     simulations = np.random.binomial(n_cookies, p_burnt, N_simulations)\n",
    "    \n",
    "#     # Calculate the observed statistic (number of burnt cookies)\n",
    "#     observed_statistic = 15\n",
    "    \n",
    "#     # Calculate the p-value (one-tailed test)\n",
    "#     p_value = np.mean(simulations >= observed_statistic)\n",
    "    \n",
    "#     return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cookies_null_hypothesis():\n",
    "    \"\"\"\n",
    "    Returns a list of reasonable choices for the null hypothesis.\n",
    "    \"\"\"\n",
    "    return [1, 2]\n",
    "\n",
    "def cookies_p_value(N_simulations):\n",
    "    \"\"\"\n",
    "    Estimates the p-value of the investigation by simulating the null hypothesis N times.\n",
    "    \n",
    "    Parameters:\n",
    "    N_simulations (int): The number of simulations to run.\n",
    "    \n",
    "    Returns:\n",
    "    float: The estimated p-value.\n",
    "    \"\"\"\n",
    "    # Define the parameters of the null hypothesis\n",
    "    n_cookies = 250\n",
    "    p_burnt = 0.04\n",
    "    \n",
    "    # Simulate the null hypothesis N times\n",
    "    simulations = np.random.binomial(n_cookies, p_burnt, N_simulations)\n",
    "    \n",
    "    # Calculate the observed statistic (number of burnt cookies)\n",
    "    observed_statistic = 15\n",
    "    \n",
    "    # Calculate the p-value (one-tailed test)\n",
    "    p_value = np.mean(simulations >= observed_statistic)\n",
    "    \n",
    "    return float(p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q3</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q3 results: All test cases passed!"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gotten our feet wet with hypothesis testing, let's take a closer look at how to choose null and alternative hypotheses and test statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 ‚Äì Tires üöó\n",
    "\n",
    "A tire manufacturer, TritonTire, claims that their tires are so good, they will bring a Toyota Highlander from 60 MPH to a complete stop in under 106 feet, 95% percent of the time.\n",
    "\n",
    "Now, you own a Toyota Highlander equipped with TritonTire tires, and you decide to test this claim. You take your car to an empty Vons parking lot, speed up to exactly 60 MPH, hit the brakes, and measure the stopping distance. As illegal as it is, you repeat this process 50 times and find that **you stopped in under 106 feet only 47 of the 50 times**.\n",
    "\n",
    "Livid, you call TritonTire and say that their claim is false. They say, no, that you were just unlucky: your experiment is consistent with their claim. But they didn't realize that they are dealing with a *data scientist* üßë‚Äçüî¨.\n",
    "\n",
    "To settle the matter, you decide to unleash the power of the hypothesis test. The following three subparts ask you to answer a total of four select-all multiple choice questions.\n",
    "\n",
    "#### Question 4.1: `car_null_hypothesis` and `car_alt_hypothesis`\n",
    "\n",
    "You will set up a hypothesis test in order to test your suspicion that the tires are are actually worse than claimed. Which of the following are valid null and alternative hypotheses for this hypothesis test?\n",
    "\n",
    "1. The tires will stop your car in under 106 feet exactly 95% of the time.\n",
    "0. The tires will stop your car in under 106 feet less than 95% of the time.\n",
    "0. The tires will stop your car in under 106 feet greater than 95% of the time.\n",
    "0. The tires will stop your car in more than 106 feet exactly 5% of the time.\n",
    "0. The tires will stop your car in more than 106 feet less than 5% of the time.\n",
    "0. The tires will stop your car in more than 106 feet greater than 5% of the time.\n",
    "\n",
    "Complete the implementation of the function `car_null_hypothesis`, which takes zero arguments and returns a list of integers, corresponding to the the valid null hypotheses above.\n",
    "Also complete the implementation of the function called `car_alt_hypothesis`, which takes zero arguments and returns a list of integers, corresponding to the valid alternative hypotheses above given your observation.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Question 4.2: `car_test_statistic`\n",
    "\n",
    "Which of the following are valid test statistics for our question?\n",
    "\n",
    "1. The number of times the car stopped in under 106 feet in 50 attempts.\n",
    "1. The average number of feet the car took to come to a complete stop in 50 attempts.\n",
    "1. The number of attempts it took before the car stopped in under 95 feet.\n",
    "1. The proportion of attempts in which the car stopped in under 106 feet in 50 attempts.\n",
    "\n",
    "Complete the implementation of the function `car_test_stat`, which takes zero arguments and returns a list of integers, corresponding to the valid test statistics above.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Question 4.3: `car_p_value`\n",
    "\n",
    "The p-value is the probability, under the assumption the null hypothesis is true, of observing a test statistic **equal to our observed statistic, or more extreme in the direction of the alternative hypothesis**.\n",
    "\n",
    "Why don't we just look at the probability of observing a test statistic equal to our observed statistic? That is, why is the \"more extreme in the direction of the alternative hypothesis\" part necessary?\n",
    "\n",
    "1. Because our observed test statistic isn't extreme.\n",
    "2. Because our null hypothesis isn't suggesting equality.\n",
    "3. Because the probability of finding our observed test statistic equals the probability of finding something more extreme.\n",
    "4. Because if we run more and more trials, the probability of observing any particular test statistic gets closer and closer to zero.\n",
    "\n",
    "Complete the implementation of the function `car_p_value`, which takes zero arguments and returns the correct reason as an **integer** (not a list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def car_null_hypothesis():\n",
    "#     # The null hypothesis is that the tires stop the car in under 106 feet exactly 95% of the time.\n",
    "#     return [0]\n",
    "\n",
    "# def car_alt_hypothesis():\n",
    "#     # The alternative hypothesis is that the tires stop the car in under 106 feet less than 95% of the time.\n",
    "#     return [1]\n",
    "\n",
    "# def car_test_stat():\n",
    "#     # The valid test statistics are the number of times and the proportion of attempts.\n",
    "#     return [0, 3]\n",
    "\n",
    "# def car_p_value():\n",
    "#     # The reason is that the probability of observing a specific test statistic decreases with more trials.\n",
    "#     return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def car_null_hypothesis():\n",
    "#     # The claim is that tires stop under 106 feet exactly 95% of the time\n",
    "#     return [1]  # Option 1\n",
    "\n",
    "# def car_alt_hypothesis():\n",
    "#     # We suspect the success rate is less than 95%\n",
    "#     return [2]  # Option 2\n",
    "\n",
    "# def car_test_stat():\n",
    "#     # Both count and proportion are valid test statistics\n",
    "#     return [1, 4]  # Options 1 and 4\n",
    "\n",
    "# def car_p_value():\n",
    "#     # The probability of any exact value becomes tiny with more trials\n",
    "#     return 4  # Option 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def car_null_hypothesis():\n",
    "    # The claim is that tires stop under 106 feet exactly 95% of the time\n",
    "    return [1]  # Option 1\n",
    "\n",
    "def car_alt_hypothesis():\n",
    "    # We suspect the success rate is less than 95%\n",
    "    return [2]  # Option 2\n",
    "\n",
    "def car_test_statistic():  # Fixed function name to match test case\n",
    "    # Both count and proportion are valid test statistics\n",
    "    return [1, 4]  # Options 1 and 4\n",
    "\n",
    "def car_p_value():\n",
    "    # The probability of any exact value becomes tiny with more trials\n",
    "    return 4  # Option 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q4</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q4 results: All test cases passed!"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5 ‚Äì Superheroes ü¶∏\n",
    "\n",
    "In the previous two questions, we ran hypothesis tests that didn't require us to look at stored data. In this next question, we'll return to the `heroes` DataFrame from Lab 2, which is read in from the file `data/superheroes.csv`.\n",
    "\n",
    "Our goal in this section will be to answer the question:\n",
    "\n",
    "> Are there significantly **more** \"good\" blond-haired, blue-eyed characters than the general pool of characters?\n",
    "\n",
    "To answer this question, we will conduct a hypothesis test. You choose the following null hypothesis:\n",
    "\n",
    "> The proportion of \"good\" characters among blond-haired, blue-eyed characters is equal to the proportion of \"good\" characters in the sample population from the current `heroes` DataFrame.\n",
    "\n",
    "and alternative hypothesis: \n",
    "\n",
    "> the distribution of \"good\" characters among blond-haired, blue-eyed characters is greater than the proportion of \"good\" characters in the overall population.\n",
    "\n",
    "To proceed with the hypothesis test, we will need to determine the test statistics for our test.\n",
    "\n",
    "#### `superheroes_test_statistic`\n",
    "\n",
    "Which of the following are valid test statistics for our question?\n",
    "\n",
    "1. The difference in proportions for \"good\" characters among blond-haired, blue-eyed characters and \"good\" characters in the overall population. \n",
    "1. The number of \"good\" characters that are blond-haired, blue-eyed.\n",
    "1. The proportion of blond-haired, blue-eyed characters among all \"good\" characters.\n",
    "1. The absolute difference in proportions for \"good\" characters among blond-haired, blue-eyed characters and \"good\" characters in the overall population.\n",
    "\n",
    "Complete the implementation of the function `superheros_test_stat`, which takes zero arguments and returns a list of integers, corresponding to all the valid test statistics above.\n",
    "\n",
    "---\n",
    "\n",
    "Regardless of your choice for the above question, we will use the test statistic stated below to complete the implementations of the following functions:\n",
    "\n",
    "> The proportion of \"good\" characters among blond-haired, blue-eyed characters.\n",
    "\n",
    "#### `bhbe_col`\n",
    "\n",
    "To start, complete the implementation of the function `bhbe_col`, which takes in a DataFrame like `heroes` and returns a Boolean Series that contains `True` for characters that have **both** blond hair and blue eyes, and `False` for all other characters. \n",
    "\n",
    "***Note***: If a character's hair color contains the word `'blond'`, uppercase or lowercase, we consider their hair to be blond for the purposes of this question. Similarly, if a character's eye color contains the word `'blue'`, uppercase or lowercase, we consider their eye color to be blue for the purposes of this question.\n",
    "\n",
    "Fix a significance level (i.e. p-value cutoff) of 1%.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `superheroes_observed_statistic`\n",
    "Complete the implementation of the function `superheroes_observed_statistic`, which takes in a DataFrame like `heroes` and returns the observed test statistic.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `simulate_bhbe_null` \n",
    "Complete the implementation of the function `simulate_bhbe_null`, which takes in a DataFrame like `heroes` and a positive integer `N` and returns an array of length `N`, where each element is a simulated test statistic according to the null hypothesis.\n",
    "\n",
    "***Hint***: Like in `superheroes_observed_statistic`, you'll need to use both `bhbe_col` and information in the `heroes` DataFrame to complete your simulation. Remember that you cannot use `for`-loops in this question.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `superheroes_p_value` \n",
    "Complete the implementation of the function `superheroes_p_value`, which takes in DataFrame like `heroes` and returns a list where:\n",
    "* The first element is the p-value for the hypothesis test, using 100,000 simulations.\n",
    "* The second element is `'Reject'` if you reject the null hypothesis and `'Fail to reject'` if you fail to reject the null hypothesis, at the 1% significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def superheroes_p_value:\n",
    "#     return\n",
    "# def simulate_bhbe_null:\n",
    "#     return\n",
    "# def superheroes_observed_statistic:\n",
    "#     return\n",
    "# def bhbe_col:\n",
    "#     return\n",
    "# def superheroes_test:\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def superheroes_test_statistic():\n",
    "    return [1, 2, 4]\n",
    "def bhbe_col(df):\n",
    "    return (df['Hair color'].str.contains('blond', case=False)) & (df['Eye color'].str.contains('blue', case=False))\n",
    "def superheroes_observed_statistic(df):\n",
    "    bhbe = bhbe_col(df)\n",
    "    return (df['Alignment'] == 'good')[bhbe].mean()\n",
    "\n",
    "def simulate_bhbe_null(df, N):\n",
    "    bhbe = bhbe_col(df)\n",
    "    good_proportion = (df['Alignment'] == 'good').mean()\n",
    "    simulated_stats = np.random.binomial(n=bhbe.sum(), p=good_proportion, size=N) / bhbe.sum()\n",
    "    return simulated_stats\n",
    "\n",
    "def superheroes_p_value(df):\n",
    "    observed_stat = superheroes_observed_statistic(df)\n",
    "    simulated_stats = simulate_bhbe_null(df, 100000)\n",
    "    p_value = (simulated_stats >= observed_stat).mean()\n",
    "    if p_value < 0.01:\n",
    "        return [p_value, 'Reject']\n",
    "    else:\n",
    "        return [p_value, 'Fail to reject']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def superheroes_test_statistic():\n",
    "    # The valid test statistics are:\n",
    "    # 1. Difference in proportions (Option 1)\n",
    "    # 4. Absolute difference in proportions (Option 4)\n",
    "    return [1, 4]\n",
    "\n",
    "def bhbe_col(df):\n",
    "    \"\"\"Return a Boolean Series for blond-haired, blue-eyed characters.\"\"\"\n",
    "    return (df['Hair color'].str.contains('blond', case=False, na=False)) & \\\n",
    "           (df['Eye color'].str.contains('blue', case=False, na=False))\n",
    "\n",
    "def superheroes_observed_statistic(df):\n",
    "    \"\"\"Calculate the observed proportion of 'good' characters among blond-haired, blue-eyed characters.\"\"\"\n",
    "    bhbe = bhbe_col(df)\n",
    "    good_bhbe = (df.loc[bhbe, 'Alignment'] == 'good').mean()\n",
    "    return good_bhbe\n",
    "\n",
    "def simulate_bhbe_null(df, N):\n",
    "    \"\"\"Simulate N test statistics under the null hypothesis.\"\"\"\n",
    "    bhbe = bhbe_col(df)\n",
    "    good_proportion = (df['Alignment'] == 'good').mean()\n",
    "    # Simulate the number of 'good' characters in the bhbe group under the null\n",
    "    simulated_stats = np.random.binomial(n=bhbe.sum(), p=good_proportion, size=N) / bhbe.sum()\n",
    "    return simulated_stats\n",
    "\n",
    "def superheroes_p_value(df):\n",
    "    \"\"\"Compute the p-value and decide whether to reject the null hypothesis.\"\"\"\n",
    "    observed_stat = superheroes_observed_statistic(df)\n",
    "    simulated_stats = simulate_bhbe_null(df, 100000)\n",
    "    \n",
    "    # Calculate the p-value based on the proportion of simulations >= observed_stat\n",
    "    p_value = (simulated_stats >= observed_stat).mean()\n",
    "    \n",
    "    # Determine the hypothesis test result based on the 1% significance level\n",
    "    decision = 'Reject' if p_value < 0.01 else 'Fail to reject'\n",
    "    \n",
    "    # Ensure the p-value is returned as a standard Python float, not np.float64\n",
    "    return [float(p_value), decision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "superheroes_fp = Path('data') / 'superheroes.csv'\n",
    "heroes = pd.read_csv(superheroes_fp, index_col=0)\n",
    "bhbe_out = bhbe_col(heroes)\n",
    "obs_stat_out = superheroes_observed_statistic(heroes)\n",
    "simulate_bhbe_out = simulate_bhbe_null(heroes, 10)\n",
    "pval_out = superheroes_p_value(heroes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00011, 'Reject']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pval_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q5</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q5 results: All test cases passed!"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" markdown=\"1\">\n",
    "\n",
    "**You cannot use `for`-loops in Part 1 (Time Series Data) or Part 2 (Hypothesis Testing), but you can use them in Part 3 (Permutation Testing).**\n",
    "\n",
    "</div>\n",
    "\n",
    "## Part 3: Permutation Testing\n",
    "\n",
    "Recall, hypothesis tests answer questions of the form:\n",
    "\n",
    "> I have a population distribution, and I have one sample. Does this sample look like it was drawn from the population?\n",
    "\n",
    "While permutation tests answer questions of the form:\n",
    "\n",
    "> I have two samples, but no information about any population distributions. Do these samples look like they were drawn from the same population?\n",
    "\n",
    "Keep this in mind while working on this last part of the lab.\n",
    "\n",
    "<br>\n",
    "\n",
    "[Skittles](https://en.wikipedia.org/wiki/Skittles_(confectionery)) üç¨ are made in two locations in the United States: Yorkville, Illinois and Waco, Texas. In these factories, Skittles of different colors are made separately by different machines and combined/packaged into bags for sale. The **tab-separated file** `data/skittles.tsv` contains the contents of 468 bags of Skittles.\n",
    "\n",
    "Throughout this question, we will compare the color distribution of Skittles between bags made in the Yorkville factory and bags made in the Waco factory. Most people have preferences for their favorite flavor, and there is a surprising amount of variation among the distribution of flavors in each bag.\n",
    "\n",
    "Look at the variation by bag in the dataset below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red</th>\n",
       "      <th>orange</th>\n",
       "      <th>yellow</th>\n",
       "      <th>green</th>\n",
       "      <th>purple</th>\n",
       "      <th>Factory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>Yorkville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>Yorkville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   red  orange  yellow  green  purple    Factory\n",
       "0   10      15      11      7      18  Yorkville\n",
       "1    5      12      17     15      10  Yorkville\n",
       "2   16      11      15     11       9       Waco\n",
       "3   15       8      13     16       7       Waco\n",
       "4   11      14      20      8       7       Waco"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skittles_fp = Path('data') / 'skittles.tsv'\n",
    "skittles = pd.read_csv(skittles_fp, sep='\\t')\n",
    "skittles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(468, 6)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skittles.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 ‚Äì Orange Skittles üü†\n",
    "\n",
    "First, you will investigate if the machine that mixes together the Skittles of different colors might favor one color over another. Use a permutation test to assess whether, on average, bags made in Yorkville have the same number of orange skittles as bags made in Waco. Do this by implementing the functions described below.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `diff_of_means`\n",
    "\n",
    "Complete the implementation of the function `diff_of_means`, which takes in a DataFrame like `skittles` and returns the **absolute difference** between the **mean** number of orange Skittles per bag from Yorkville and the **mean** number of orange Skittles per bag from Waco.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `simulate_null`\n",
    "\n",
    "Complete the implementation of the function `simulate_null`, which takes in a DataFrame like `skittles` and returns one simulated instance of the test statistic under the null hypothesis. Note that this will involve shuffling the `'Factory'` column!\n",
    "\n",
    "<br>\n",
    "\n",
    "#### `color_p_value`\n",
    "\n",
    "Complete the implementation of the function `color_p_value`, which takes in a DataFrame like `skittles` and calculates the p-value for the permutation test using 1000 trials.\n",
    "\n",
    "<br>\n",
    "\n",
    "Plot the observed statistic, along with the histogram for the simulated distribution, to check your work.\n",
    "\n",
    "***Note***: In all functions, the default argument for `col` is `'orange'`. Your functions should still work for any color so that you can call it in later questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_of_means(data, col='orange'):\n",
    "    yorkville_mean = data[data['Factory'] == 'Yorkville'][col].mean()\n",
    "    waco_mean = data[data['Factory'] == 'Waco'][col].mean()\n",
    "    return abs(yorkville_mean - waco_mean)\n",
    "\n",
    "def simulate_null(data, col='orange'):\n",
    "    shuffled_data = data.copy()\n",
    "    shuffled_data['Factory'] = np.random.permutation(shuffled_data['Factory'])\n",
    "    return diff_of_means(shuffled_data, col)\n",
    "\n",
    "def color_p_value(data, col='orange', num_trials=1000):\n",
    "    observed_diff = diff_of_means(data, col)\n",
    "    simulated_diffs = [simulate_null(data, col) for _ in range(num_trials)]\n",
    "    p_value = np.mean(np.array(simulated_diffs) >= observed_diff)\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "# cell may take about 1-2 minutes to execute to completion\n",
    "skittles_fp = Path('data') / 'skittles.tsv'\n",
    "skittles = pd.read_csv(skittles_fp, sep='\\\\t', engine='python')\n",
    "q6_diff_of_means_out = diff_of_means(skittles)\n",
    "q6_simulate_null_out = simulate_null(skittles)\n",
    "q6_pval_out = color_p_value(skittles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q6</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q6 results: All test cases passed!"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7 ‚Äì Generalizing to all colors üî¥üü†üü°üü¢üü£\n",
    "\n",
    "While your `color_p_value` function used a default color of `'orange'`, it should also work for all other colors of Skittles, meaning you can run the same permutation test from Question 7 on all colors of Skittles. Call `color_p_value` on all colors of Skittles to find which colors differ the most between the two locations on average. \n",
    "\n",
    "Then, complete the implementation of the function `ordered_colors`, which returns a list of five ordered pairs, each of the form `('color', p_value)`. For example, your list might look like `[('pink', 0.000), ('brown', 0.025), ...]`. \n",
    "\n",
    "The list should be **hard-coded**, meaning that you should run your permutation tests in your notebook, not in your `.py` file. The list should also be sorted in **increasing order of p-value**. Make sure your p-values are rounded to **3 decimal places**.\n",
    "\n",
    "Even though there is randomness in the color composition in each bag, this list gives the likelihood that the machines have a systematic, meaningful, difference in how they blend the colors in each bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('orange', np.float64(0.053))\n",
      "('yellow', np.float64(0.0))\n",
      "('red', np.float64(0.241))\n",
      "('green', np.float64(0.504))\n",
      "('purple', np.float64(0.977))\n"
     ]
    }
   ],
   "source": [
    "here = [\n",
    "        ('orange', color_p_value(skittles,\"orange\")),\n",
    "        ('yellow', color_p_value(skittles,\"yellow\")),\n",
    "        ('red', color_p_value(skittles,\"red\")),\n",
    "        ('green', color_p_value(skittles,\"green\")),\n",
    "        ('purple', color_p_value(skittles,\"purple\"))\n",
    "    ]\n",
    "for i in here:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ordered_colors():\n",
    "    \"\"\"Return a hard-coded list of ('color', p_value) pairs, sorted by p-value.\"\"\"\n",
    "    return [\n",
    "        ('orange', (0.056)),\n",
    "        ('yellow', (0.0)),\n",
    "        ('red', (0.24)),\n",
    "        ('green', (0.476)),\n",
    "        ('purple', (0.976))\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "q7_out = ordered_colors()\n",
    "q7_colors = {'green', 'orange', 'purple', 'red', 'yellow'}\n",
    "q7_test_colors = [x[0] for x in q7_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong style='color: red;'><pre style='display: inline;'>q7</pre> results:</strong></p><p><strong><pre style='display: inline;'>q7 - 1</pre> result:</strong></p><pre>    Test case passed!</pre><p><strong><pre style='display: inline;'>q7 - 2</pre> result:</strong></p><pre>    Test case passed!</pre><p><strong><pre style='display: inline;'>q7 - 3</pre> result:</strong></p><pre>    Test case passed!</pre><p><strong><pre style='display: inline;'>q7 - 4</pre> result:</strong></p><pre>    Test case passed!</pre><p><strong><pre style='display: inline;'>q7 - 5</pre> result:</strong></p><pre>    Trying:\n",
       "        bool(np.isclose(q7_out[0][1], 0.0))\n",
       "    Expecting:\n",
       "        True\n",
       "    **********************************************************************\n",
       "    Line 1, in q7 4\n",
       "    Failed example:\n",
       "        bool(np.isclose(q7_out[0][1], 0.0))\n",
       "    Expected:\n",
       "        True\n",
       "    Got:\n",
       "        False\n",
       "</pre><p><strong><pre style='display: inline;'>q7 - 5</pre> message:</strong> smallest pval</p>"
      ],
      "text/plain": [
       "q7 results:\n",
       "    q7 - 1 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q7 - 2 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q7 - 3 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q7 - 4 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q7 - 5 result:\n",
       "        Trying:\n",
       "            bool(np.isclose(q7_out[0][1], 0.0))\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q7 4\n",
       "        Failed example:\n",
       "            bool(np.isclose(q7_out[0][1], 0.0))\n",
       "        Expected:\n",
       "            True\n",
       "        Got:\n",
       "            False\n",
       "\n",
       "    q7 - 5 message: smallest pval"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 ‚Äì Overall distributions üè≠\n",
    "\n",
    "Now, suppose you would like to assess whether the two locations make similar amounts of each color overall. That is, suppose we:\n",
    "* Combine and count up all the Skittles of each color that were made in Yorkville (e.g. 14303 total red skittles, 9091 total green skittles, etc.).\n",
    "* Combine and count up all the Skittles of each color that were made in Waco.\n",
    "\n",
    "Now, suppose you would like to assess whether the two locations make similar proportions of each color overall. That is, suppose we:\n",
    "* Calculate the proportion of each Skittles color that were made in Yorkville (e.g. out of the 14704 skittles made, 19.8% of them are red,  18.9% of them are green, etc.).\n",
    "* Calculate the proportion of each Skittles color that were made in Waco.\n",
    "\n",
    "**Are these distributions of colors similar?** Is the variation among the bags due to each factory making different amounts of each color?\n",
    "\n",
    "Use a permutation test to assess whether the distribution of colors of Skittles made in Yorkville is statistically significantly different than those made in Waco. Set a significance level (i.e. p-value cutoff) of 0.01 and determine whether you can reject a null hypothesis that answers the question above using a permutation test with 1000 trials. For your test statistic, use the **total variation distance (TVD)**.\n",
    "\n",
    "Refer to the end of [Lecture 6](https://dsc80.com/resources/lectures/lec06/lec06.html#Permutation-testing-meets-TVD) to see an example of a [permutation test](https://www.inferentialthinking.com/chapters/12/Comparing_Two_Samples.html) that uses the [TVD](https://inferentialthinking.com/chapters/11/2/Multiple_Categories.html) as the test statistic. Some guidance:\n",
    "\n",
    "- Our previous permutation tests have compared the mean number of (say) orange Skittles in Yorkville bags to the mean number number of orange Skittles in Waco bags. The role of shuffling was to randomly assign bags to Yorkville and Waco.\n",
    "- In this permutation test, we are **still** shuffling to randomly assign bags to Yorkville and Waco. The only difference is that after we randomly assign each bag to a factory, we will compute the **distribution** of colors among the two factories and find the TVD between those two distributions.\n",
    "\n",
    "**Your job**: Complete the implementation of the function `same_color_distribution`, which takes in no arguments and outputs a hard-coded **tuple** with the p-value and whether you `'Reject'` or `'Fail to reject'` the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red</th>\n",
       "      <th>orange</th>\n",
       "      <th>yellow</th>\n",
       "      <th>green</th>\n",
       "      <th>purple</th>\n",
       "      <th>Factory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>Yorkville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>Yorkville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>Yorkville</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>468 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     red  orange  yellow  green  purple    Factory\n",
       "0     10      15      11      7      18  Yorkville\n",
       "1      5      12      17     15      10  Yorkville\n",
       "2     16      11      15     11       9       Waco\n",
       "3     15       8      13     16       7       Waco\n",
       "4     11      14      20      8       7       Waco\n",
       "..   ...     ...     ...    ...     ...        ...\n",
       "463   11      11      12     13      11       Waco\n",
       "464   17      10       8     11      12       Waco\n",
       "465    9      14      12     10      15       Waco\n",
       "466   12      14      11     10      10       Waco\n",
       "467   11       8      12     13      15  Yorkville\n",
       "\n",
       "[468 rows x 6 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skittles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_tvd(p, q):\n",
    "#     \"\"\"Calculate the Total Variation Distance between two distributions.\"\"\"\n",
    "#     return 0.5 * np.sum(np.abs(p - q))\n",
    "\n",
    "# def same_color_distribution():\n",
    "#     # Load the data (assuming the data is in 'data/skittles.csv')\n",
    "#     data = skittles\n",
    "\n",
    "#     # Separate Skittles by factory\n",
    "#     yorkville = data[data['Factory'] == 'Yorkville']\n",
    "#     waco = data[data['Factory'] == 'Waco']\n",
    "\n",
    "#     # Calculate the proportion of each color made by both factories\n",
    "#     yorkville_proportions = yorkville['Color'].value_counts(normalize=True).sort_index()\n",
    "#     waco_proportions = waco['Color'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "#     # Compute the observed TVD\n",
    "#     observed_tvd = calculate_tvd(yorkville_proportions, waco_proportions)\n",
    "\n",
    "#     # Perform permutation test with 1000 trials\n",
    "#     combined = data.copy()\n",
    "#     n_trials = 1000\n",
    "#     tvd_values = np.zeros(n_trials)\n",
    "\n",
    "#     for i in range(n_trials):\n",
    "#         # Shuffle the factory labels\n",
    "#         shuffled_labels = np.random.permutation(combined['Factory'])\n",
    "#         combined['Shuffled Factory'] = shuffled_labels\n",
    "\n",
    "#         # Compute new proportions for the shuffled labels\n",
    "#         yorkville_shuffled = combined[combined['Shuffled Factory'] == 'Yorkville']\n",
    "#         waco_shuffled = combined[combined['Shuffled Factory'] == 'Waco']\n",
    "\n",
    "#         yorkville_shuffled_prop = yorkville_shuffled['Color'].value_counts(normalize=True).sort_index()\n",
    "#         waco_shuffled_prop = waco_shuffled['Color'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "#         # Ensure both distributions have the same color order\n",
    "#         tvd_values[i] = calculate_tvd(yorkville_shuffled_prop, waco_shuffled_prop)\n",
    "\n",
    "#     # Calculate the p-value\n",
    "#     p_value = (tvd_values >= observed_tvd).mean()\n",
    "\n",
    "#     # Determine whether to reject the null hypothesis\n",
    "#     decision = 'Reject' if p_value < 0.01 else 'Fail to reject'\n",
    "\n",
    "#     # Return a hard-coded result\n",
    "#     return (round(p_value, 3), decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>red</th>\n",
       "      <th>orange</th>\n",
       "      <th>yellow</th>\n",
       "      <th>green</th>\n",
       "      <th>purple</th>\n",
       "      <th>Factory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>Yorkville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>Yorkville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>Waco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>Yorkville</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>468 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     red  orange  yellow  green  purple    Factory\n",
       "0     10      15      11      7      18  Yorkville\n",
       "1      5      12      17     15      10  Yorkville\n",
       "2     16      11      15     11       9       Waco\n",
       "3     15       8      13     16       7       Waco\n",
       "4     11      14      20      8       7       Waco\n",
       "..   ...     ...     ...    ...     ...        ...\n",
       "463   11      11      12     13      11       Waco\n",
       "464   17      10       8     11      12       Waco\n",
       "465    9      14      12     10      15       Waco\n",
       "466   12      14      11     10      10       Waco\n",
       "467   11       8      12     13      15  Yorkville\n",
       "\n",
       "[468 rows x 6 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skittles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_tvd(p, q):\n",
    "#     \"\"\"Calculate the Total Variation Distance between two distributions.\"\"\"\n",
    "#     return 0.5 * np.sum(np.abs(p - q))\n",
    "\n",
    "# def same_color_distribution():\n",
    "#     # Load the data\n",
    "#     # data = skittles\n",
    "#     data = pd.read_csv('data/skittles.tsv')\n",
    "\n",
    "    \n",
    "#     # Aggregate the counts of each color by factory\n",
    "#     yorkville_totals = data[data['Factory'] == 'Yorkville'].iloc[:, :-1].sum()\n",
    "#     waco_totals = data[data['Factory'] == 'Waco'].iloc[:, :-1].sum()\n",
    "\n",
    "#     # Compute proportions of each color for both factories\n",
    "#     yorkville_proportions = yorkville_totals / yorkville_totals.sum()\n",
    "#     waco_proportions = waco_totals / waco_totals.sum()\n",
    "\n",
    "#     # Calculate the observed TVD between the two factories\n",
    "#     observed_tvd = calculate_tvd(yorkville_proportions, waco_proportions)\n",
    "\n",
    "#     # Perform permutation test with 1000 trials\n",
    "#     n_trials = 1000\n",
    "#     tvd_values = np.zeros(n_trials)\n",
    "\n",
    "#     for i in range(n_trials):\n",
    "#         # Shuffle the factory labels\n",
    "#         shuffled_labels = np.random.permutation(data['Factory'])\n",
    "\n",
    "#         # Assign new factory labels and aggregate by the shuffled labels\n",
    "#         yorkville_shuffled_totals = data[shuffled_labels == 'Yorkville'].iloc[:, :-1].sum()\n",
    "#         waco_shuffled_totals = data[shuffled_labels == 'Waco'].iloc[:, :-1].sum()\n",
    "\n",
    "#         # Compute new proportions for shuffled data\n",
    "#         yorkville_shuffled_prop = yorkville_shuffled_totals / yorkville_shuffled_totals.sum()\n",
    "#         waco_shuffled_prop = waco_shuffled_totals / waco_shuffled_totals.sum()\n",
    "\n",
    "#         # Compute TVD for the shuffled data\n",
    "#         tvd_values[i] = calculate_tvd(yorkville_shuffled_prop, waco_shuffled_prop)\n",
    "\n",
    "#     # Calculate the p-value\n",
    "#     p_value = (tvd_values >= observed_tvd).mean()\n",
    "\n",
    "#     # Determine whether to reject the null hypothesis\n",
    "#     decision = 'Reject' if p_value < 0.01 else 'Fail to reject'\n",
    "\n",
    "#     # Return a hard-coded result (rounded p-value and decision)\n",
    "#     return (round(p_value, 3), decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_tvd(p, q):\n",
    "    \"\"\"Calculate the Total Variation Distance between two distributions.\"\"\"\n",
    "    return 0.5 * np.sum(np.abs(p - q))\n",
    "\n",
    "def same_color_distribution():\n",
    "    # Load the data and ensure correct parsing\n",
    "    data = pd.read_csv('data/skittles.tsv', sep='\\t')\n",
    "    \n",
    "    # Check column names and clean up any whitespace\n",
    "    data.columns = data.columns.str.strip()\n",
    "\n",
    "    # Aggregate the counts of each color by factory\n",
    "    yorkville_totals = data[data['Factory'] == 'Yorkville'].iloc[:, :-1].sum()\n",
    "    waco_totals = data[data['Factory'] == 'Waco'].iloc[:, :-1].sum()\n",
    "\n",
    "    # Compute proportions of each color for both factories\n",
    "    yorkville_proportions = yorkville_totals / yorkville_totals.sum()\n",
    "    waco_proportions = waco_totals / waco_totals.sum()\n",
    "\n",
    "    # Calculate the observed TVD between the two factories\n",
    "    observed_tvd = calculate_tvd(yorkville_proportions, waco_proportions)\n",
    "\n",
    "    # Perform permutation test with 1000 trials\n",
    "    n_trials = 1000\n",
    "    tvd_values = np.zeros(n_trials)\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        # Shuffle the factory labels\n",
    "        shuffled_labels = np.random.permutation(data['Factory'])\n",
    "\n",
    "        # Assign new factory labels and aggregate by the shuffled labels\n",
    "        yorkville_shuffled_totals = data[shuffled_labels == 'Yorkville'].iloc[:, :-1].sum()\n",
    "        waco_shuffled_totals = data[shuffled_labels == 'Waco'].iloc[:, :-1].sum()\n",
    "\n",
    "        # Compute new proportions for shuffled data\n",
    "        yorkville_shuffled_prop = yorkville_shuffled_totals / yorkville_shuffled_totals.sum()\n",
    "        waco_shuffled_prop = waco_shuffled_totals / waco_shuffled_totals.sum()\n",
    "\n",
    "        # Compute TVD for the shuffled data\n",
    "        tvd_values[i] = calculate_tvd(yorkville_shuffled_prop, waco_shuffled_prop)\n",
    "\n",
    "    # Calculate the p-value\n",
    "    p_value = (tvd_values >= observed_tvd).mean()\n",
    "\n",
    "    # Determine whether to reject the null hypothesis\n",
    "    decision = 'Reject' if p_value < 0.01 else 'Fail to reject'\n",
    "\n",
    "    # Return a hard-coded result (rounded p-value and decision)\n",
    "    return (round(p_value, 3), decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "q8_out = same_color_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong style='color: red;'><pre style='display: inline;'>q8</pre> results:</strong></p><p><strong><pre style='display: inline;'>q8 - 1</pre> result:</strong></p><pre>    Test case passed!</pre><p><strong><pre style='display: inline;'>q8 - 2</pre> result:</strong></p><pre>    Test case passed!</pre><p><strong><pre style='display: inline;'>q8 - 3</pre> result:</strong></p><pre>    Trying:\n",
       "        q8_out[1] in ['Fail to Reject', 'Reject']\n",
       "    Expecting:\n",
       "        True\n",
       "    **********************************************************************\n",
       "    Line 1, in q8 2\n",
       "    Failed example:\n",
       "        q8_out[1] in ['Fail to Reject', 'Reject']\n",
       "    Expected:\n",
       "        True\n",
       "    Got:\n",
       "        False\n",
       "</pre><p><strong><pre style='display: inline;'>q8 - 3</pre> message:</strong> wrong output type at index 1</p><p><strong><pre style='display: inline;'>q8 - 4</pre> result:</strong></p><pre>    Test case passed!</pre><p><strong><pre style='display: inline;'>q8 - 5</pre> result:</strong></p><pre>    Test case passed!</pre>"
      ],
      "text/plain": [
       "q8 results:\n",
       "    q8 - 1 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q8 - 2 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q8 - 3 result:\n",
       "        Trying:\n",
       "            q8_out[1] in ['Fail to Reject', 'Reject']\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q8 2\n",
       "        Failed example:\n",
       "            q8_out[1] in ['Fail to Reject', 'Reject']\n",
       "        Expected:\n",
       "            True\n",
       "        Got:\n",
       "            False\n",
       "\n",
       "    q8 - 3 message: wrong output type at index 1\n",
       "\n",
       "    q8 - 4 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q8 - 5 result:\n",
       "        Test case passed!"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9 ‚Äì Permutation testing vs. hypothesis testing üß™\n",
    "\n",
    "In each of the following scenarios, decide  whether  a  permutation test is appropriate to determine if there is a  significant difference between the quantities described. If a permutation test is appropriate, mark `'P'`. Otherwise, mark `'H'`.\n",
    "\n",
    "Record your answers in the function `perm_vs_hyp`, which outputs a list of length 5, containing the values `'P'` and `'H'`.\n",
    "\n",
    "1. Compare the DSC 80 pass rate between second years and third years who take the class.\n",
    "2. Compare the proportion of Data Science majors who have completed DSC 80 and the proportion of Data Science minors who have completed DSC 80.\n",
    "3. Compare the proportion of students who have iPhones to the proportion of students who have Android phones (for simplicity, assume that all students either have an iPhone or an Android).\n",
    "4. In DSC 80, we ask all students whether they liked DSC 40A or DSC 40B more. Compare the proportion of students who preferred DSC 40A to the proportion who preferred DSC 40B.\n",
    "5. A sales company has two branches, one in city A and one in city B. You want to compare the amount of sales revenue brought in by employees at the two branches.\n",
    "\n",
    "***Hint***: Think about the type of data you would collect in each case, and how you would simulate new data under the null hypothesis. It will be useful to refer to the explanation at the start of Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_vs_hyp():\n",
    "    return ['P', 'P', 'P', 'H', 'P']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "q9_out = perm_vs_hyp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q9</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q9 results: All test cases passed!"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You're done Lab 4! üèÅ\n",
    "\n",
    "As a reminder, all of the work you want to submit needs to be in `lab.py`.\n",
    "\n",
    "To ensure that all of the work you want to submit is in `lab.py`, we've included a script named `lab-validation.py` in the lab folder. You shouldn't edit it, but instead, you should call it from the command line (e.g. the Terminal) to test your work.\n",
    "\n",
    "Once you've finished the lab, you should open the command line and run, in the directory for this lab:\n",
    "\n",
    "```\n",
    "python lab-validation.py\n",
    "```\n",
    "\n",
    "**This will run all of the `grader.check` cells that you see in this notebook, but only using the code in `lab.py` ‚Äì that is, it doesn't look at any of the code in this notebook. If all of your `grader.check` cells pass in this notebook but not all of them pass in your command line with the above command, then you likely have code in your notebook that isn't in your `lab.py`!**\n",
    "\n",
    "You can also use `lab-validation.py` to test individual questions. For instance,\n",
    "\n",
    "```\n",
    "python lab-validation.py q1 q2 q4\n",
    "```\n",
    "\n",
    "will run the `grader.check` cells for Questions 1, 2, and 4 ‚Äì again, only using the code in `lab.py`. [This video](https://www.loom.com/share/0ea254b85b2745e59322b5e5a8692e91?sid=5acc92e6-0dfe-4555-9b6a-8115b6a52f99) how to use the script as well.\n",
    "\n",
    "Once `python lab-validation.py` shows that you're passing all test cases, you're ready to submit your `lab.py` (and only your `lab.py`) to Gradescope. Once submitting to Gradescope, make sure to stick around until all test cases pass.\n",
    "\n",
    "There is also a call to `grader.check_all()` below in _this_ notebook, but make sure to also follow the steps above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q1 results: All test cases passed!\n",
       "\n",
       "q2 results: All test cases passed!\n",
       "\n",
       "q3 results: All test cases passed!\n",
       "\n",
       "q4 results: All test cases passed!\n",
       "\n",
       "q5 results: All test cases passed!\n",
       "\n",
       "q6 results: All test cases passed!\n",
       "\n",
       "q7 results:\n",
       "    q7 - 1 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q7 - 2 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q7 - 3 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q7 - 4 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q7 - 5 result:\n",
       "        Trying:\n",
       "            bool(np.isclose(q7_out[0][1], 0.0))\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q7 4\n",
       "        Failed example:\n",
       "            bool(np.isclose(q7_out[0][1], 0.0))\n",
       "        Expected:\n",
       "            True\n",
       "        Got:\n",
       "            False\n",
       "\n",
       "    q7 - 5 message: smallest pval\n",
       "\n",
       "q8 results:\n",
       "    q8 - 1 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q8 - 2 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q8 - 3 result:\n",
       "        Trying:\n",
       "            q8_out[1] in ['Fail to Reject', 'Reject']\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q8 2\n",
       "        Failed example:\n",
       "            q8_out[1] in ['Fail to Reject', 'Reject']\n",
       "        Expected:\n",
       "            True\n",
       "        Got:\n",
       "            False\n",
       "\n",
       "    q8 - 3 message: wrong output type at index 1\n",
       "\n",
       "    q8 - 4 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q8 - 5 result:\n",
       "        Test case passed!\n",
       "\n",
       "q9 results:\n",
       "    q9 - 1 result:\n",
       "        Trying:\n",
       "            len(q9_out) == 5\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q9 0\n",
       "        Failed example:\n",
       "            len(q9_out) == 5\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q9 0[0]>\", line 1, in <module>\n",
       "                len(q9_out) == 5\n",
       "                ^^^^^^^^^^^\n",
       "            TypeError: object of type 'NoneType' has no len()\n",
       "\n",
       "    q9 - 1 message: output length should be 5\n",
       "\n",
       "    q9 - 2 result:\n",
       "        Trying:\n",
       "            set(q9_out) <= set(['P', 'H'])\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q9 1\n",
       "        Failed example:\n",
       "            set(q9_out) <= set(['P', 'H'])\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q9 1[0]>\", line 1, in <module>\n",
       "                set(q9_out) <= set(['P', 'H'])\n",
       "                ^^^^^^^^^^^\n",
       "            TypeError: 'NoneType' object is not iterable\n",
       "\n",
       "    q9 - 2 message: output contains answers other than P or H"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check_all()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q1_result) == 433\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> bool(q1_result.loc[393, \"Time\"] > 9)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> bool(q1_result.loc[457, 'Time'] == 4)\nTrue",
         "failure_message": "check user 457",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> bool(q1_result.loc[458, 'Time'] > 50)\nTrue",
         "failure_message": "check user 457",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> len(set(q1_result.index)) == 433\nTrue",
         "failure_message": "check number of unique users",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q2_result) == 433\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> bool(np.isclose(q2_result.loc[466], 0.24250681198910082))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(cookies_null_hypothesis(), list)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> set(cookies_null_hypothesis()).issubset({1, 2, 3, 4})\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> 0 < cookies_p_value(1000) < 0.6\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> set(car_null_hypothesis()) <= set(range(1, 7))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> set(car_alt_hypothesis()) <= set(range(1, 7))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> set(car_test_statistic()) <= set(range(1, 5))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> car_p_value() in set(range(1, 6))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> set(superheroes_test_statistic()) <= set(range(1, 5))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(bhbe_out, pd.Series)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> bhbe_out.dtype == np.dtype('bool')\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> bool(bhbe_out.sum() == 93)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> bool(0.5 <= obs_stat_out <= 1.0)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(simulate_bhbe_out, np.ndarray)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> simulate_bhbe_out.shape[0] == 10\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> bool(((0.45 <= simulate_bhbe_out) & (simulate_bhbe_out <= 1)).all())\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> len(pval_out) == 2\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> 0 <= pval_out[0] <= 1\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> pval_out[1] in ['Reject', 'Fail to reject']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(q6_diff_of_means_out, float)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> isinstance(q6_simulate_null_out, float)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> bool(0 <= q6_simulate_null_out <= 1.0)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> isinstance(q6_pval_out, float)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> bool(0 <= q6_pval_out <= 1)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> bool(q6_diff_of_means_out > 0)\nTrue",
         "failure_message": "should be greater than zero",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q7_out) == 5\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> set([x[0] for x in q7_out]) == q7_colors\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> all([isinstance(x[1], float) for x in q7_out])\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> q7_test_colors.index('green') > q7_test_colors.index('yellow')\nTrue",
         "failure_message": "yellow less than green",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> bool(np.isclose(q7_out[0][1], 0.0))\nTrue",
         "failure_message": "smallest pval",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8": {
     "name": "q8",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(q8_out, tuple)\nTrue",
         "failure_message": "tuple",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> isinstance(q8_out[0], float)\nTrue",
         "failure_message": "wrong output type at index 0",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> q8_out[1] in ['Fail to Reject', 'Reject']\nTrue",
         "failure_message": "wrong output type at index 1",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> bool(np.isclose(q8_out[0], 0.005, atol=0.25))\nTrue",
         "failure_message": "p-value, approximate within 0.25",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> bool(np.isclose(q8_out[0], 0.005, atol=0.5))\nTrue",
         "failure_message": "p-value, approximate within 0.5",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9": {
     "name": "q9",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(q9_out) == 5\nTrue",
         "failure_message": "output length should be 5",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> set(q9_out) <= set(['P', 'H'])\nTrue",
         "failure_message": "output contains answers other than P or H",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

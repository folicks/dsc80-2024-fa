{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 â€“ DataFrames and Grouping\n",
    "\n",
    "## DSC 80, Fall 2024\n",
    "\n",
    "### Due Date: Friday, October 11th at 11:59PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Welcome to the second DSC 80 lab this quarter!\n",
    "\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and Markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding will be done in an accompanying `lab.py` file that is imported into the current notebook, and **you will only submit that `lab.py` file**, not this notebook!\n",
    "\n",
    "Some additional guidelines:\n",
    "- **Unlike in DSC 10, labs will have both public tests and hidden tests.** The bulk of your grade will come from your scores on hidden tests, which you will only see on Gradescope after the assignment deadline.\n",
    "- **Do not change the function names in the `lab.py` file!** The functions in the `lab.py` file are how your assignment is graded, and they are graded by their name. If you changed something you weren't supposed to, you can find the original code in the [course GitHub repository](https://github.com/dsc-courses/dsc80-2024-fa).\n",
    "- Notebooks are nice for testing and experimenting with different implementations before designing your function in your `lab.py` file. You can write code here, but make sure that all of your real work is in the `lab.py` file, since that's all you're submitting.\n",
    "- You are encouraged to write your own additional helper functions to solve the lab, as long as they also end up in `lab.py`.\n",
    "\n",
    "**To ensure that all of the work you want to submit is in `lab.py`, we've included a script named `lab-validation.py` in the lab folder. You shouldn't edit it, but instead, you should call it from the command line (e.g. the Terminal) to test your work.** More details on its usage are given at the bottom of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing code from `lab.py`**:\n",
    "\n",
    "* Below, we import the `.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "    \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<b>The only question in this lab that you're allowed to use a loop in is Question 3.</b> There, you may use a <code>for</code>-loop to loop over the columns in the input DataFrame, but not the rows. <b>If you use a <code>for</code>-loop or <code>while</code>-loop in any other question, you may lose points!</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tricky Pandas ðŸ¤”\n",
    "\n",
    "Sometimes, `pandas` gives you weird outputs that you may not expect. The next question walks you through a few examples that might surprise you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "The following subparts all require you to define a function and return a number that is the answer to a multiple-choice question. You may need to write code and experiment with DataFrames to arrive at your answers.\n",
    "\n",
    "#### `trick_me`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trick_me():\n",
    "#     TODO\n",
    "#         find function that cast a df to csv\n",
    "#         then answer wat the difference bw the \n",
    "#         raw construction put together with collections\n",
    "#         making a copy of it in df form\n",
    "#             IS IT THE SAME OR NOT\n",
    "#             return 1,2,3\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Frank', 'Grace', 'Henry', 'Ivy', 'Julia']\n",
      "Original DataFrame:\n",
      "Index(['Name', 'Age'], dtype='object')\n",
      "\n",
      "Read-in DataFrame:\n",
      "Index(['Name', 'Age'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def trick_me():\n",
    "    # Create a dictionary with duplicate column names\n",
    "    data = {\n",
    "        'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "        'Name': ['Frank', 'Grace', 'Henry', 'Ivy', 'Julia'],\n",
    "        'Age': [25, 30, 35, 40, 45]\n",
    "    }\n",
    "    \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    tricky_1 = pd.DataFrame(data)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    tricky_1.to_csv('tricky_1.csv', index=False)\n",
    "    \n",
    "    # Read the CSV file back into another DataFrame\n",
    "    tricky_2 = pd.read_csv('tricky_1.csv')\n",
    "\n",
    "    print(data[\"Name\"])\n",
    "        # see note\n",
    "    # Compare the DataFrames\n",
    "    print(\"Original DataFrame:\")\n",
    "    print(tricky_1.columns)\n",
    "    print(\"\\nRead-in DataFrame:\")\n",
    "    print(tricky_2.columns)\n",
    "\n",
    "# Call the function\n",
    "trick_me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def trick_me():\n",
    "    # Create a list of tuples representing the DataFrame\n",
    "    data = [\n",
    "        ('Alice', 'Frank', 25),\n",
    "        ('Bob', 'Grace', 30),\n",
    "        ('Charlie', 'Henry', 35),\n",
    "        ('David', 'Ivy', 40),\n",
    "        ('Eve', 'Julia', 45)\n",
    "    ]\n",
    "    \n",
    "    # Convert the list of tuples to a DataFrame\n",
    "    tricky_1 = pd.DataFrame(data, columns=['Name', 'Name', 'Age'])\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    tricky_1.to_csv('tricky_1.csv', index=False)\n",
    "    \n",
    "    # Read the CSV file back into another DataFrame\n",
    "    tricky_2 = pd.read_csv('tricky_1.csv')\n",
    "    \n",
    "    # Compare the DataFrames\n",
    "    # print(\"Original DataFrame:\")\n",
    "    # print(tricky_1.columns)\n",
    "    # print(\"\\nRead-in DataFrame:\")\n",
    "    # print(tricky_2.columns)\n",
    "    return 1\n",
    "# Call the function\n",
    "trick_me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tricky_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtricky_2\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tricky_2' is not defined"
     ]
    }
   ],
   "source": [
    "tricky_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "`trick_me` should not take any arguments. \n",
    "<br>\n",
    "\n",
    "Inside the function:\n",
    "\n",
    "* Create a DataFrame named `tricky_1` that has three columns labeled `'Name'`, `'Name'`, and `'Age'`. `tricky_1` should have 5 rows; the values are up to you.\n",
    "* Save the DataFrame to a `.csv` file called `'tricky_1.csv'` without the index.\n",
    "* Now create another DataFrame, named `tricky_2`, by reading in the file `'tricky_1.csv'`. What are your observations?\n",
    "\n",
    "  1. It was not possible to create a DataFrame with the duplicate columns.\n",
    "  2. `tricky_1` and `tricky_2` have the same column names.\n",
    "  3. `tricky_1` and `tricky_2` have different column names.\n",
    "   \n",
    "Your function should return `1`, `2`, or `3`, answering the above question.\n",
    "\n",
    "<font color='red'>**Hints:** Dictionaries can only contain unique keys, so you <b>cannot</b> use them to declare a DataFrame with duplicate column names. Find other ways to create a DataFrame with duplicate column names.</font>\n",
    "\n",
    "<br>\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trick_bool():\n",
    "#     TODO\n",
    "#         OPTIONAL make a df with four columns named Bool\n",
    "\n",
    "#     IGGGG?!!!? the bracket filters below give the same dimensions every time?!?!?!?\n",
    "#         bools[True]\n",
    "#         bools[[True, True, False, False]]\n",
    "#         bools[[True, False]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    True: [1, 2, 3, 4],   # First True column\n",
    "    True: [5, 6, 7, 8],   # Second True column (will be renamed to True.1)\n",
    "    False: [9, 10, 11, 12], # First False column\n",
    "    False: [13, 14, 15, 16] \n",
    "]\n",
    "    # Second False column (will be renamed to False.1)\n",
    "\n",
    "bools = pd.DataFrame(data)\n",
    "\n",
    "print(bools[True])\n",
    "\n",
    "print(\"\\nShape of bools[[True, True, False, False]]:\")\n",
    "print(bools[[True, True, False, False]])\n",
    "\n",
    "print(\"\\nShape of bools[[True, False]]:\")\n",
    "print(bools[[True, False]].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [1, 2, 3, 4],   # First True column\n",
    "    [5, 6, 7, 8],   # Second True column (will be renamed to True.1)\n",
    "    [9, 10, 11, 12], # First False column\n",
    "    [13, 14, 15, 16] # Second False column (will be renamed to False.1)\n",
    "]\n",
    "\n",
    "# Create the DataFrame with specified column names\n",
    "bools = pd.DataFrame(data, columns=[True, True, False, False])\n",
    "\n",
    "print(bools[True])\n",
    "\n",
    "print(\"\\nShape of bools[[True, True, False, False]]:\")\n",
    "print(bools[[True, True, False, False]])\n",
    "\n",
    "print(\"\\nShape of bools[[True, False]]:\")\n",
    "print(bools[[True, False]].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `trick_bool`\n",
    "`trick_bool` should not take any arguments.\n",
    "\n",
    "To determine the correct answer from the list below, you should follow the steps outlined by experimenting in **the notebook** (or in the Terminal by running `python`). Outside the function:\n",
    "\n",
    "* Create a DataFrame named `bools` that has four columns: `True`, `True`, `False`, `False`. Each column name should be Boolean.\n",
    "* `bools` should have 4 rows; the values are up to you.\n",
    "* Predict the shape of the DataFrame that results by running each of the three lines of code below. Pick a corresponding answer from the given list. Your function should return a list with three numbers, one for each line.\n",
    "* You should be able to answer without running any code, but feel free to run code to check your answer.\n",
    "* **Your function should not do anything other than return a hardcoded list.**\n",
    "\n",
    "```py\n",
    "bools[True]\n",
    "bools[[True, True, False, False]]\n",
    "bools[[True, False]]\n",
    "```\n",
    "    \n",
    "Answer choices:\n",
    "1. DataFrame: 2 columns, 1 row\n",
    "2. DataFrame: 2 columns, 2 rows\n",
    "3. DataFrame: 2 columns, 3 rows\n",
    "4. DataFrame: 2 columns, 4 rows\n",
    "5. DataFrame: 3 columns, 1 rows\n",
    "6. DataFrame: 3 columns, 2 rows\n",
    "7. DataFrame: 3 columns, 3 rows\n",
    "8. DataFrame: 3 columns, 4 rows\n",
    "9. DataFrame: 4 columns, 1 rows\n",
    "10. DataFrame: 4 columns, 2 rows\n",
    "11. DataFrame: 4 columns, 3 rows\n",
    "12. DataFrame: 4 columns, 4 rows\n",
    "13. Error\n",
    "\n",
    "**Hints:** Refer to the previous hint for `trick_me`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trick_bool():\n",
    "    return [10, 4, 13]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = trick_me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "trick_ans = trick_bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Summary Statistics ðŸ“Š\n",
    "\n",
    "In this question you will define two general purpose functions that make it easy to qualitatively assess the contents of a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Complete the implementation of the function `population_stats`, which takes in a DataFrame `df` and returns a DataFrame indexed by the columns of `df`, with the following columns:\n",
    "* `'num_nonnull'`, which contains the number of non-null entries in each column.\n",
    "* `'prop_nonnull'`, which contains the proportion of entries in each column that are non-null.\n",
    "* `'num_distinct'`, which contains the number of distinct non-null entries in each column.\n",
    "* `'prop_distinct'`, which contains the proportion of non-null entries that are distinct in each column.\n",
    "       \n",
    "For example, if `df` has a column named `'ages'` with the following elements (note that `np.nan` is a null value):\n",
    "       \n",
    "```py\n",
    "[2, 2, 2, np.nan, 5, 7, 5, 10, 11, np.nan]\n",
    "```\n",
    "\n",
    "Then:\n",
    "- `'num_nonnull'` is 8, and `'prop_nonnull'` is $\\frac{8}{10}$ = 0.8.\n",
    "- There are six distinct entries, `[2, 5, 7, 10, 11, np.nan]`, but only 5 of them are non-null. So the number of distinct non-null entries, `'num_distinct'`, is 5.\n",
    "- There are 5 distinct non-null entries, and there are 8 total non-null entries, so `'prop_distinct'` is $\\frac{5}{8}$ = 0.625.\n",
    "\n",
    "Putting it all together, `population_stats(df).loc['ages']` should be a Series containing the numbers 8, 0.8, 5, and 0.625."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def population_stats(df):\n",
    "    '''\n",
    "        TODO \n",
    "            ##is how to get the return series to transform to a single cell?\n",
    "        \n",
    "            do this 2 times in .apply form of the passed df\n",
    "            a bunch of groupbys nesscary ?\n",
    "                blah.groupby blah .count?\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(powers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def population_stats(df):\n",
    "    stats = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Calculate number of non-null entries\n",
    "        num_nonnull = df[col].count()\n",
    "        \n",
    "        # Calculate proportion of non-null entries\n",
    "        prop_nonnull = num_nonnull / len(df)\n",
    "        \n",
    "        # Calculate number of distinct non-null entries\n",
    "        distinct_values = df[col].dropna().unique()\n",
    "        num_distinct = len(distinct_values)\n",
    "        \n",
    "        # Calculate proportion of distinct non-null entries\n",
    "        prop_distinct = num_distinct / num_nonnull\n",
    "        \n",
    "        # Store the statistics in a dictionary\n",
    "        stats[col] = pd.Series([num_nonnull, prop_nonnull, num_distinct, prop_distinct])\n",
    "    \n",
    "    # Create the final DataFrame\n",
    "    result = pd.DataFrame.from_dict(stats, orient='index').T\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "df = pd.DataFrame({\n",
    "    'ages': [2, 2, 2, np.nan, 5, 7, 5, 10, 11, np.nan],\n",
    "    'heights': [160, 170, 180, 165, 175, 185, 190, 155, 150, 162]\n",
    "})\n",
    "\n",
    "stats_df = population_stats(df)\n",
    "(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def population_stats(df):\n",
    "    # Calculate num_nonnull\n",
    "    num_nonnull = df.count()\n",
    "    \n",
    "    # Calculate prop_nonnull\n",
    "    prop_nonnull = num_nonnull / len(df)\n",
    "    \n",
    "    # Calculate num_distinct (excluding null values)\n",
    "    num_distinct = df.nunique()\n",
    "    \n",
    "    # Calculate prop_distinct\n",
    "    prop_distinct = num_distinct / num_nonnull\n",
    "    \n",
    "    # Create the result DataFrame\n",
    "    result = pd.DataFrame({\n",
    "        'num_nonnull': num_nonnull,\n",
    "        'prop_nonnull': prop_nonnull,\n",
    "        'num_distinct': num_distinct,\n",
    "        'prop_distinct': prop_distinct\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "# Create a sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'ages': [2, 2, 2, np.nan, 5, 7, 5, 10, 11, np.nan]\n",
    "})\n",
    "\n",
    "# Call the function\n",
    "result = population_stats(df)\n",
    "\n",
    "# Print the result\n",
    "(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "pop_data = np.random.choice(range(10), size=(100, 4))\n",
    "df_pop = pd.DataFrame(pop_data, columns='A B C D'.split())\n",
    "out_pop = population_stats(df_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pop[\"C\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "    \n",
    "Complete the implementation of the function `most_common`, which takes in a DataFrame `df` and a number `N` and returns a DataFrame of the `N` most-common values and their counts for each column of `df`. Any column with fewer than `N` distinct values should contain `np.nan` in those entries.\n",
    "\n",
    "For example, consider the DataFrame shown on the left. This DataFrame is a subset of `salaries`, a larger DataFrame containing information on employees in the City of San Diego. The subset below contains two of the original columns: `'Job Title'` which contains job titles for employees, and `'status'` which denotes whether the employee works a full time position (`'FT'`) or a part time position (`'PT'`). On the right, the return value of `most_common(salaries, N=5)` is shown.\n",
    "\n",
    "You can assume that there are no ties in our hidden tests.\n",
    "\n",
    "<table><tr>\n",
    "    <td><img src=\"data/imgs/dataframe.png\" width=\"90%\"/></td>\n",
    "    <td><img src=\"data/imgs/most_common.png\" width=\"90%\"/></td>\n",
    "</tr></table>\n",
    "\n",
    "***Note***: Remember, to access values in a Series based on their integer position, including when slicing for the first `N` values in a Series, you **must** use `.iloc` followed by square brackets. If you just use square brackets and don't use `.iloc`, you may not see the results you expect!\n",
    "\n",
    "***Hint***: You may find that initializing an empty DataFrame with `N` rows and adding columns to it is useful in your implementation.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<b>Remember, the only question in this lab that you're allowed to use a loop in is Question 3 â€“ that's this question.</b> Here, you may use a <code>for</code>-loop to loop over the columns in the input DataFrame (<code>df</code>), but not the rows. <b>If you use a <code>for</code>-loop or <code>while</code>-loop in any other question, you may lose points!</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def most_common(df, N):\n",
    "    # Initialize an empty DataFrame with N rows and index 0 to N-1\n",
    "    result = pd.DataFrame(index=range(N))\n",
    "    \n",
    "    for column in df.columns:\n",
    "        # Get value counts for the column\n",
    "        value_counts = df[column].value_counts()\n",
    "            # i don't think this choice of value\n",
    "        # Get the N most common values and their counts\n",
    "        top_N = value_counts.iloc[:N]\n",
    "        \n",
    "        # Create a DataFrame for this column\n",
    "        column_df = pd.DataFrame({\n",
    "            f'{column}_value': top_N.index,\n",
    "            f'{column}_count': top_N.values\n",
    "        })\n",
    "        \n",
    "        # If there are fewer than N distinct values, fill with NaN\n",
    "        if len(column_df) < N:\n",
    "            column_df = column_df.reindex(range(N), fill_value=np.nan)\n",
    "        \n",
    "        # Add the column data to the result DataFrame\n",
    "        result[f'{column}_value'] = column_df[f'{column}_value']\n",
    "        result[f'{column}_count'] = column_df[f'{column}_count']\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(df, N):\n",
    "    # Initialize an empty DataFrame with N rows and index 0 to N-1\n",
    "    result = pd.DataFrame(index=range(N))\n",
    "    \n",
    "    for column in df.columns:\n",
    "        # Get value counts for the column\n",
    "        value_counts = df[column].value_counts()\n",
    "        \n",
    "        # Get the N most common values and their counts\n",
    "        top_N = value_counts.iloc[:N]\n",
    "        \n",
    "        # Create a DataFrame for this column\n",
    "        column_df = pd.DataFrame({\n",
    "            f'{column}_values': top_N.index,\n",
    "            f'{column}_counts': top_N.values\n",
    "        })\n",
    "        \n",
    "        # If there are fewer than N distinct values, fill with NaN\n",
    "        if len(column_df) < N:\n",
    "            column_df = column_df.reindex(range(N), fill_value=np.nan)\n",
    "        \n",
    "        # Add the column data to the result DataFrame\n",
    "        result[f'{column}_values'] = column_df[f'{column}_values']\n",
    "        result[f'{column}_counts'] = column_df[f'{column}_counts']\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_data = pd.read_csv('tricky_1.csv')\n",
    "dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common(dummy_data,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "common_data = np.random.choice(range(10), size=(100, 2))\n",
    "common_df = pd.DataFrame(common_data, columns='A B'.split())\n",
    "common_out = most_common(common_df, N=3)\n",
    "common_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Superheroes ðŸ¦¸\n",
    "\n",
    "The questions below analyze a dataset of superheroes found in the `data` directory. One of the datasets lists the attributes of each superhero, while the other is a *Boolean* DataFrame describing which superheroes have which superpowers. Note, the datasets contain information on both **good** superheroes, as well as **bad** superheroes (AKA villains).\n",
    "\n",
    "If you took DSC 10 in Fall 2022, this dataset may seem familiar â€“ it was used for the Final Project that quarter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Let's start working with the `powers` dataset, which you can see in `data/superheroes_powers.csv`. \n",
    "\n",
    "Complete the implementation of the function `super_hero_powers`, which takes in a DataFrame like `powers` and returns a list with the following three entries:\n",
    "\n",
    "1. The name of the superhero with the greatest number of superpowers.\n",
    "2. Identify the most common superpower among superheroes who can fly, other than `'Flight'` itself.\n",
    "3. The name of the most common superpower among superheroes with only one superpower.\n",
    "\n",
    "You should **not** be hard-coding your answers in this question; your function should work on any DataFrame similar to `powers`. In each case, you can assume the answer is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_fp = Path('data') / 'superheroes_powers.csv'\n",
    "stats_df = pd.read_csv(super_fp) # trial \n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_hero_powers(mode_df):\n",
    "    # 1. Superhero with the greatest number of superpowers\n",
    "    most_powers = mode_df.iloc[:, 1:].sum(axis=1).idxmax()\n",
    "    most_powers_name = powers.loc[most_powers, 'hero_names']\n",
    "    # 2. Most common superpower among flying superheroes (excluding 'Flight')\n",
    "    flying_heroes = mode_df[mode_df['Flight'] == True]\n",
    "    common_flying_power = flying_heroes.iloc[:, 1:].drop('Flight', axis=1).sum().idxmax()\n",
    "\n",
    "    # 3. Most common superpower among heroes with only one superpower\n",
    "    single_power_heroes = mode_df[mode_df.iloc[:, 1:].sum(axis=1) == 1]\n",
    "    common_single_power = single_power_heroes.iloc[:, 1:].sum().idxmax()\n",
    "\n",
    "    return [most_powers_name, common_flying_power, common_single_power]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_return = super_hero_powers(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(verify_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in verify_return:\n",
    "    print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "    # 1st\n",
    "    # do that one method from dsc10 \n",
    "        # sum the total of 1's across the entry\n",
    "        # find the index level of the max of that sum found\n",
    "            # across the entries\n",
    "    # 2nd\n",
    "        # filter new df of \"flight\" heroes\n",
    "        # find the max of the 1's collected in each column\n",
    "    # 3rd\n",
    "        # filter for one power heroes\n",
    "        # do a count \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_hero_powers(powers):\n",
    "    # Assume the first column contains hero names, and the rest are powers\n",
    "    hero_name_col = powers.columns[0]\n",
    "    power_cols = powers.columns[1:]\n",
    "\n",
    "    # 1. Superhero with the greatest number of superpowers\n",
    "    most_powers_name = powers[power_cols].sum(axis=1).idxmax()\n",
    "\n",
    "    # 2. Most common superpower among flying superheroes (excluding 'Flight')\n",
    "    flight_col = power_cols[power_cols.str.lower() == 'flight']\n",
    "    if not flight_col.empty:\n",
    "        flying_heroes = powers[powers[flight_col.iloc[0]] == True]\n",
    "        other_powers = power_cols[power_cols != flight_col.iloc[0]]\n",
    "        common_flying_power = flying_heroes[other_powers].sum().idxmax()\n",
    "    else:\n",
    "        common_flying_power = None\n",
    "\n",
    "    # 3. Most common superpower among heroes with only one superpower\n",
    "    single_power_heroes = powers[powers[power_cols].sum(axis=1) == 1]\n",
    "    common_single_power = single_power_heroes[power_cols].sum().idxmax()\n",
    "\n",
    "    return [most_powers_name, common_flying_power, common_single_power]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_hero_powers(powers):\n",
    "    # Assume the first column contains hero names, and the rest are powers\n",
    "    hero_name_col = powers.columns[0]\n",
    "    power_cols = powers.columns[1:]\n",
    "\n",
    "    # 1. Superhero with the greatest number of superpowers\n",
    "    most_powers_name = powers.loc[powers[power_cols].sum(axis=1).idxmax(), hero_name_col]\n",
    "\n",
    "    # 2. Most common superpower among flying superheroes (excluding 'Flight')\n",
    "    flight_col = power_cols[power_cols.str.lower() == 'flight']\n",
    "    if not flight_col.empty:\n",
    "        flying_heroes = powers[powers[flight_col.iloc[0]] == True]\n",
    "        other_powers = power_cols[power_cols != flight_col.iloc[0]]\n",
    "        common_flying_power = flying_heroes[other_powers].sum().idxmax()\n",
    "    else:\n",
    "        common_flying_power = None\n",
    "\n",
    "    # 3. Most common superpower among heroes with only one superpower\n",
    "    single_power_heroes = powers[powers[power_cols].sum(axis=1) == 1]\n",
    "    common_single_power = single_power_heroes[power_cols].sum().idxmax()\n",
    "\n",
    "    return [most_powers_name, common_flying_power, common_single_power]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_hero_powers(powers):\n",
    "    # 1. Superhero with the greatest number of superpowers\n",
    "    most_powers_name = powers.loc[powers.iloc[:, 1:].sum(axis=1).idxmax(), 'hero_names']\n",
    "\n",
    "    # 2. Most common superpower among flying superheroes (excluding 'Flight')\n",
    "    flying_heroes = powers[powers['Flight'] == True]\n",
    "    common_flying_power = flying_heroes.drop(['hero_names', 'Flight'], axis=1).sum().idxmax()\n",
    "\n",
    "    # 3. Most common superpower among heroes with only one superpower\n",
    "    single_power_heroes = powers[powers.iloc[:, 1:].sum(axis=1) == 1]\n",
    "    common_single_power = single_power_heroes.iloc[:, 1:].sum().idxmax()\n",
    "\n",
    "    return [most_powers_name, common_flying_power, common_single_power]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spectre', 'Super Strength', 'Intelligence']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "super_fp = Path('data') / 'superheroes_powers.csv'\n",
    "powers = pd.read_csv(super_fp)\n",
    "super_out = super_hero_powers(powers)\n",
    "super_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q4</pre></strong> passed!</p>"
      ],
      "text/plain": [
       "q4 results: All test cases passed!"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "In the notebook, load in the dataset in `data/superheroes.csv` as a DataFrame and explore it. Call your `population_stats` function from Question 2 on the DataFrame. You should notice that there are very few actually null (`np.nan`) values, but there are many entries that **should** be null, because they're missing.\n",
    "\n",
    "Complete the implementation of the function `clean_heroes`, which takes in a DataFrame like the one created from `superheroes.csv` and returns a new DataFrame with all of the missing values replaced with `np.nan`.\n",
    "\n",
    "After cleaning the superheroes dataset with `clean_heroes`, run `population_stats` on it again. As a result of the cleaning, population_stats should show that there are more null values.\n",
    "\n",
    "<mark>***Note***: Most of the work in this question is identifying how the missing values are stored in the DataFrame. The implementation of the function should only take one line.<mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MORE HEROES THAN POWERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heroes.iloc[288]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(heroes[[\"Weight\"]].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heroes[[\"Weight\"]].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_issue = heroes[[\"Skin color\"]].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(heroes[[\"Skin color\"]].dropna().stack().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(heroes[\"Skin color\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pelicular_nan = population_stats(heroes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common(pelicular_nan,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_heroes(df):\n",
    "    # TODO\n",
    "        # find wat the demonstrated form of null\n",
    "    return None\n",
    "    # missing values will appear that ARENT the \n",
    "    # assignment specific null type but they will be different\n",
    "    # type then NaN \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_heroes(df):\n",
    "    #TODO\n",
    "        # replace instances of \"-\" in Skin color with np.nan\n",
    "        # GOAL do so in one line\n",
    "    \n",
    "    # return df.replace(['-', 'null', ''], np.nan)\n",
    "    return df.replace(['-', 'null', ''], np.nan).mask((df['Height'] < 0) | (df['Weight'] < 0))\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_df = clean_heroes(heroes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_stats(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "superheroes_fp = Path('data') / 'superheroes.csv'\n",
    "heroes = pd.read_csv(superheroes_fp, index_col=0)\n",
    "clean_out = clean_heroes(heroes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we have displayed the first 10 rows of the cleaned DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_out.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Using the **cleaned** superhero data, we will now generate some insights.\n",
    "\n",
    "Complete the implementation of the function `super_hero_stats`, which takes no arguments and returns a list of length 6 containing your answers to the questions below. **Your answers should be hard-coded in the function.**\n",
    "\n",
    "0. What is the name of the tallest `'Mutant'` with `'No Hair'`?\n",
    "1. Among the publishers who have more than 5 characters, which publisher has the highest proportion of human characters? If there is a tie, return the publisher whose name is first alphabetically. We define a character to be human if their `'Race'` is exactly the string `'Human'`; for instance, a `'Race'` of `'Human / Radiation'` is non-human for the purposes of this question.\n",
    "2. Among the characters whose `'Height'`s we know, who is taller on average â€“ `'good'` characters or `'bad'` characters?\n",
    "3. Which publisher has a greater proportion of `'bad'` characters â€“ `'Marvel Comics'` or `'DC Comics'`?\n",
    "4. Which `'Publisher'` that isn't `'Marvel Comics'` or `'DC Comics'` has the most characters? Consider all characters whose `'Publisher'` we know â€“ that is, don't drop rows because they have null values in other columns.\n",
    "5. There is only one character that is **both** more one standard deviation above the mean in height and more than one standard deviation below the mean in weight. What is their name?\n",
    "\n",
    "***Note***: When calculating your answers, do not rows based on null values.\n",
    "\n",
    "***Note***: Although you'll be writing code to find the answers, you should not include your code in your `.py` file. Just return a hard-coded list with your answers to the 6 questions; all 6 elements in the list should be strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search 1\n",
    "# cleaned_df[\"Race\"].unique()\n",
    "tallest_mutant = cleaned_df[(cleaned_df['Race'] == 'Mutant') & (cleaned_df['Hair color'] == 'No Hair')].sort_values('Height', ascending=False).iloc[0]['name']\n",
    "tallest_mutant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search 2\n",
    "# cleaned_df[[\"Publisher\",\"Race\"]]\n",
    "publishers = cleaned_df['Publisher'].value_counts()\n",
    "\n",
    "publishers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df[\"Publisher\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "publishers_over_5 = publishers[publishers > 5].index\n",
    "# publishers_over_5\n",
    "human_proportions = cleaned_df[cleaned_df['Publisher'].isin(publishers_over_5)].groupby('Publisher').apply(lambda x: (x['Race'] == 'Human').mean()).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "highest_human_proportion = human_proportions.index[0]\n",
    "highest_human_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search 3\n",
    "\n",
    "good_avg_height = cleaned_df[cleaned_df['Alignment'] == 'good']['Height'].mean()\n",
    "bad_avg_height = cleaned_df[cleaned_df['Alignment'] == 'bad']['Height'].mean()\n",
    "taller_alignment = 'good' if good_avg_height > bad_avg_height else 'bad'\n",
    "taller_alignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search 4\n",
    "marvel_bad_prop = cleaned_df[cleaned_df['Publisher'] == 'Marvel Comics']['Alignment'].value_counts(normalize=True)['bad']\n",
    "dc_bad_prop = cleaned_df[cleaned_df['Publisher'] == 'DC Comics']['Alignment'].value_counts(normalize=True)['bad']\n",
    "more_bad_publisher = 'Marvel Comics' if marvel_bad_prop > dc_bad_prop else 'DC Comics'\n",
    "more_bad_publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search 5\n",
    "\n",
    "other_publisher = cleaned_df[~cleaned_df['Publisher'].isin(['Marvel Comics', 'DC Comics'])]['Publisher'].value_counts().index[0]\n",
    "other_publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned_df[cleaned_df[\"Publisher\"]!= (\"Marvel Comics\" or \"DC Comics\")][\"Publisher\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height_mean, height_std = cleaned_df['Height'].mean(), cleaned_df['Height'].std()\n",
    "weight_mean, weight_std = cleaned_df['Weight'].mean(), cleaned_df['Weight'].std()\n",
    "outlier = cleaned_df[(cleaned_df['Height'] > height_mean + height_std) & (cleaned_df['Weight'] < weight_mean - weight_std)]['name'].iloc[0]\n",
    "outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def super_hero_stats():\n",
    "    return [\n",
    "        'Onslaught',\n",
    "        'Dark Horse Comics',\n",
    "        'bad',\n",
    "        'Marvel Comics',\n",
    "        'Dark Horse Comics',\n",
    "        'Groot'\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "stats_out = super_hero_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: High Potential Individuals ðŸ“ˆ\n",
    "\n",
    "Last year, the United Kingdom ðŸ‡¬ðŸ‡§ announced a new [\"High Potential Individual\" visa](https://www.lexology.com/library/detail.aspx?g=41fa64ec-9272-468c-bdcb-8002745a754f), which allows graduates of universities ranked in the Top 50 globally to move to the UK without a job lined up. This visa has been a subject of much debate, in part due to how much rankings play a role. (Rest assured, UCSD is on the list!)\n",
    "\n",
    "In this section, you will analyze a dataset of university rankings, collected from  [here](https://www.kaggle.com/datasets/mylesoneill/world-university-rankings?datasetId=) (though we have pre-processed and modified the original dataset for the purposes of this question). Our version of the dataset is stored in `data/universities_unified.csv`.\n",
    "\n",
    "Columns:\n",
    "* `'world_rank'`: world rank of the institution\n",
    "* `'institution'`: name of the institution\n",
    "* `'national_rank'`: rank within the nation, formatted as `'country, rank'`\n",
    "* `'quality_of_education'`: rank by quality of education\n",
    "* `'alumni_employment'`: rank by alumni employment\n",
    "* `'quality_of_faculty'`: rank by quality of faculty\n",
    "* `'publications'`: rank by publications\n",
    "* `'influence'`: rank by influence\n",
    "* `'citations'`: rank by number of citations\n",
    "* `'broad_impact'`: rank by broad impact\n",
    "* `'patents'`: rank by number of patents\n",
    "* `'score'`: overall score of the institution, out of 100\n",
    "* `'control'`: whether the university is public or private\n",
    "* `'city'`: city in which the institution is located\n",
    "* `'state'`: state in which the institution is located"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "There are (still) a few aspects of the dataset we need to clean before it's ready for analysis.\n",
    "\n",
    "#### `clean_universities`\n",
    "\n",
    "Complete the implementation of the function `clean_universities`, which takes in the raw rankings DataFrame and returns a cleaned DataFrame, cleaned according to the following information:\n",
    "\n",
    "- Some `'institution'` names contain `'\\n'` characters (e.g. `'University of California\\nSan Diego'`). Replace all instances of `'\\n'` with `', '` (a comma and a space) in the `'institution'` column.\n",
    "\n",
    "- Change the data type of the `'broad_impact'` column to `int`.\n",
    "\n",
    "* Split `'national_rank'` into two columns, `'nation'` and `'national_rank_cleaned'`, where:\n",
    "    * `'nation'` is the country (or its dependency) indicated in the first part of `'national_rank'`. \n",
    "        * Note that there are **3** countries that appear under different names for different schools. For all 3 of these countries, you should pick **the name that is longer** and use that name for every occurrence of the country. One of the 3 countries is **`'Czech Republic'`**, which also appears as **`'Czechia'`** â€“ since these refer to the same country and `'Czech Republic'` is longer, all instances of either name should be replaced with `'Czech Republic'`. You need to find the other 2 countries on your own. \n",
    "        * As is mentioned below, your function will only be tested on the DataFrame in `data/universities_unified.csv`, so you only need to change these 3 country names.\n",
    "    * `'national_rank_cleaned'` is the integer in the latter part of `'national_rank'`. Make sure that the data type of this column is `int`. \n",
    "    * Don't include the original `'national_rank'` column in the output DataFrame.\n",
    "* Create a Boolean column `'is_r1_public'`. This column should contain `True` if a university is public and classified as R1 and `False` otherwise. Treat `np.nan`s as False. **Note that in the raw DataFrame, a university is classified as R1 if and only if it has non-null values in all of the following columns: `'control'`, `'city'`, and `'state'`.**\n",
    "    - Read [this page](https://en.wikipedia.org/wiki/List_of_research_universities_in_the_United_States) to learn more about what it means for a university to be classified as R1.\n",
    "    \n",
    "**The only dataset your function will be tested on is `data/universities_unified.csv`; you don't need to worry about other hidden test sets.** In addition, please return a *copy* of the original DataFrame; don't modify the original.\n",
    "\n",
    "<br>\n",
    "\n",
    "Now, we can do some basic exploration.\n",
    "\n",
    "#### `university_info`\n",
    "\n",
    "Complete the implementation of the function `university_info`, which takes in the **cleaned** DataFrame outputted by `clean_universities` and returns the following values in a list:\n",
    "* Among `'state'`s with three or more `'institution'`s in the dataset, the `'state'` whose universities have the lowest mean `'score'`.\n",
    "* The proportion of the `'institution'`s in the top 100 for which the `'quality of faculty'` ranking is also in the top 100.\n",
    "* The number of `'state'`s where at least 50% of the `'institution'`s are private (i.e. have an `'is_r1_public'` of `False`).\n",
    "* The lowest-ranked (worst) `'institution'` in the world, according to `'world_rank'`, that is the highest-ranked (best) university in its nation (i.e., it has a `'national_rank_cleaned'` of 1).\n",
    "\n",
    "You can assume there are no ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_raw_data = df\n",
    "uni_raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_universities(df):\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # Replace '\\n' with ', ' in the 'institution' column\n",
    "    cleaned_df['institution'] = cleaned_df['institution'].str.replace('\\n', ', ')\n",
    "    \n",
    "    # Change the data type of 'broad_impact' to int\n",
    "    cleaned_df['broad_impact'] = cleaned_df['broad_impact'].astype(int)\n",
    "    \n",
    "    # Split 'national_rank' into 'nation' and 'national_rank_cleaned'\n",
    "    cleaned_df[['nation', 'national_rank_cleaned']] = cleaned_df['national_rank'].str.split(', ', expand=True)\n",
    "    \n",
    "    # Replace country names\n",
    "    country_replacements = {\n",
    "        'Czechia': 'Czech Republic',\n",
    "        'UK': 'United Kingdom',\n",
    "        'USA': 'United States'\n",
    "    }\n",
    "    cleaned_df['nation'] = cleaned_df['nation'].replace(country_replacements)\n",
    "    \n",
    "    # Convert 'national_rank_cleaned' to int\n",
    "    cleaned_df['national_rank_cleaned'] = cleaned_df['national_rank_cleaned'].astype(int)\n",
    "    \n",
    "    # Drop the original 'national_rank' column\n",
    "    cleaned_df = cleaned_df.drop('national_rank', axis=1)\n",
    "    \n",
    "    # Create 'is_r1_public' column\n",
    "    cleaned_df['is_r1_public'] = (\n",
    "        (cleaned_df['control'] == 'Public') &\n",
    "        cleaned_df['city'].notna() &\n",
    "        cleaned_df['state'].notna()\n",
    "    )\n",
    "    \n",
    "    # Ensure 'is_r1_public' is False for NaN values\n",
    "    cleaned_df['is_r1_public'] = cleaned_df['is_r1_public'].fillna(False)\n",
    "    \n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_uni_data = (clean_universities(uni_raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_uni_data[\"institution\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def university_info(df):\n",
    "    result = []\n",
    "\n",
    "    # 1. State with lowest mean score (among states with 3+ institutions)\n",
    "    state_counts = df['state'].value_counts()\n",
    "    states_with_3_plus = state_counts[state_counts >= 3].index\n",
    "    state_mean_scores = df[df['state'].isin(states_with_3_plus)].groupby('state')['score'].mean()\n",
    "    lowest_score_state = state_mean_scores.idxmin()\n",
    "    result.append(lowest_score_state)\n",
    "\n",
    "    # 2. Proportion of top 100 institutions also in top 100 for quality of faculty\n",
    "    top_100 = df[df['world_rank'] <= 100]\n",
    "    prop_top_100_faculty = (top_100['quality_of_faculty'] <= 100).mean()\n",
    "    result.append(prop_top_100_faculty)\n",
    "\n",
    "    # 3. Number of states where at least 50% of institutions are private\n",
    "    state_private_ratio = df.groupby('state')['is_r1_public'].apply(lambda x: (x == False).mean())\n",
    "    states_majority_private = (state_private_ratio >= 0.5).sum()\n",
    "    result.append(states_majority_private)\n",
    "\n",
    "    # 4. Lowest world-ranked institution that is highest-ranked in its nation\n",
    "    top_national = df[df['national_rank_cleaned'] == 1]\n",
    "    worst_top_national = top_national.loc[top_national['world_rank'].idxmax(), 'institution']\n",
    "    result.append(worst_top_national)\n",
    "\n",
    "    return result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't change this cell -- it is needed for the tests to work\n",
    "fp = Path('data') / 'universities_unified.csv'\n",
    "df = pd.read_csv(fp)\n",
    "cleaned = clean_universities(df)\n",
    "info = university_info(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You're done Lab 2! ðŸ\n",
    "\n",
    "As a reminder, all of the work you want to submit needs to be in `lab.py`.\n",
    "\n",
    "To ensure that all of the work you want to submit is in `lab.py`, we've included a script named `lab-validation.py` in the lab folder. You shouldn't edit it, but instead, you should call it from the command line (e.g. the Terminal) to test your work.\n",
    "\n",
    "Once you've finished the lab, you should open the command line and run, in the directory for this lab:\n",
    "\n",
    "```\n",
    "python lab-validation.py\n",
    "```\n",
    "\n",
    "**This will run all of the `grader.check` cells that you see in this notebook, but only using the code in `lab.py` â€“ that is, it doesn't look at any of the code in this notebook. If all of your `grader.check` cells pass in this notebook but not all of them pass in your command line with the above command, then you likely have code in your notebook that isn't in your `lab.py`!**\n",
    "\n",
    "You can also use `lab-validation.py` to test individual questions. For instance,\n",
    "\n",
    "```\n",
    "python lab-validation.py q1 q4 q7\n",
    "```\n",
    "\n",
    "will run the `grader.check` cells for Questions 1, 4, and 7 â€“ again, only using the code in `lab.py`. [This video](https://www.loom.com/share/0ea254b85b2745e59322b5e5a8692e91?sid=5acc92e6-0dfe-4555-9b6a-8115b6a52f99) how to use the script as well.\n",
    "\n",
    "Once `python lab-validation.py` shows that you're passing all test cases, you're ready to submit your `lab.py` (and only your `lab.py`) to Gradescope. Once submitting to Gradescope, make sure to stick around until all test cases pass.\n",
    "\n",
    "There is also a call to `grader.check_all()` below in _this_ notebook, but make sure to also follow the steps above.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "<b>Remember, the only question in this lab that you're allowed to use a loop in is Question 3.</b> There, you may use a <code>for</code>-loop to loop over the columns in the input DataFrame, but not the rows. <b>If you use a <code>for</code>-loop or <code>while</code>-loop in any other question, you may lose points!</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q1 results:\n",
       "    q1 - 1 result:\n",
       "        Test case passed!\n",
       "\n",
       "    q1 - 2 result:\n",
       "        Trying:\n",
       "            isinstance(trick_ans, list)\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q1 1\n",
       "        Failed example:\n",
       "            isinstance(trick_ans, list)\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q1 1[0]>\", line 1, in <module>\n",
       "                isinstance(trick_ans, list)\n",
       "                           ^^^^^^^^^\n",
       "            NameError: name 'trick_ans' is not defined. Did you mean: 'trick_me'?\n",
       "\n",
       "    q1 - 3 result:\n",
       "        Trying:\n",
       "            isinstance(trick_ans[1], int)\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q1 2\n",
       "        Failed example:\n",
       "            isinstance(trick_ans[1], int)\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q1 2[0]>\", line 1, in <module>\n",
       "                isinstance(trick_ans[1], int)\n",
       "                           ^^^^^^^^^\n",
       "            NameError: name 'trick_ans' is not defined. Did you mean: 'trick_me'?\n",
       "\n",
       "q2 results:\n",
       "    q2 - 1 result:\n",
       "        Trying:\n",
       "            out_pop.index.tolist() == ['A', 'B', 'C', 'D']\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q2 0\n",
       "        Failed example:\n",
       "            out_pop.index.tolist() == ['A', 'B', 'C', 'D']\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q2 0[0]>\", line 1, in <module>\n",
       "                out_pop.index.tolist() == ['A', 'B', 'C', 'D']\n",
       "                ^^^^^^^\n",
       "            NameError: name 'out_pop' is not defined\n",
       "\n",
       "    q2 - 2 result:\n",
       "        Trying:\n",
       "            cols = ['num_nonnull', 'prop_nonnull', 'num_distinct', 'prop_distinct']\n",
       "        Expecting nothing\n",
       "        ok\n",
       "        Trying:\n",
       "            out_pop.columns.tolist() == cols\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 2, in q2 1\n",
       "        Failed example:\n",
       "            out_pop.columns.tolist() == cols\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q2 1[1]>\", line 1, in <module>\n",
       "                out_pop.columns.tolist() == cols\n",
       "                ^^^^^^^\n",
       "            NameError: name 'out_pop' is not defined\n",
       "\n",
       "    q2 - 3 result:\n",
       "        Trying:\n",
       "            bool((out_pop['num_distinct'] <= 10).all())\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q2 2\n",
       "        Failed example:\n",
       "            bool((out_pop['num_distinct'] <= 10).all())\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q2 2[0]>\", line 1, in <module>\n",
       "                bool((out_pop['num_distinct'] <= 10).all())\n",
       "                      ^^^^^^^\n",
       "            NameError: name 'out_pop' is not defined\n",
       "\n",
       "    q2 - 4 result:\n",
       "        Trying:\n",
       "            bool((out_pop['prop_nonnull'] == 1.0).all())\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q2 3\n",
       "        Failed example:\n",
       "            bool((out_pop['prop_nonnull'] == 1.0).all())\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q2 3[0]>\", line 1, in <module>\n",
       "                bool((out_pop['prop_nonnull'] == 1.0).all())\n",
       "                      ^^^^^^^\n",
       "            NameError: name 'out_pop' is not defined\n",
       "\n",
       "q3 results:\n",
       "    q3 - 1 result:\n",
       "        Trying:\n",
       "            common_out.index.tolist() == [0, 1, 2]\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q3 0\n",
       "        Failed example:\n",
       "            common_out.index.tolist() == [0, 1, 2]\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q3 0[0]>\", line 1, in <module>\n",
       "                common_out.index.tolist() == [0, 1, 2]\n",
       "                ^^^^^^^^^^\n",
       "            NameError: name 'common_out' is not defined\n",
       "\n",
       "    q3 - 2 result:\n",
       "        Trying:\n",
       "            common_out.columns.tolist() == ['A_values', 'A_counts', 'B_values', 'B_counts']\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q3 1\n",
       "        Failed example:\n",
       "            common_out.columns.tolist() == ['A_values', 'A_counts', 'B_values', 'B_counts']\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q3 1[0]>\", line 1, in <module>\n",
       "                common_out.columns.tolist() == ['A_values', 'A_counts', 'B_values', 'B_counts']\n",
       "                ^^^^^^^^^^\n",
       "            NameError: name 'common_out' is not defined\n",
       "\n",
       "    q3 - 3 result:\n",
       "        Trying:\n",
       "            bool(common_out['A_values'].isin(range(10)).all())\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q3 2\n",
       "        Failed example:\n",
       "            bool(common_out['A_values'].isin(range(10)).all())\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q3 2[0]>\", line 1, in <module>\n",
       "                bool(common_out['A_values'].isin(range(10)).all())\n",
       "                     ^^^^^^^^^^\n",
       "            NameError: name 'common_out' is not defined\n",
       "\n",
       "q4 results:\n",
       "    q4 - 1 result:\n",
       "        Trying:\n",
       "            isinstance(super_out, list)\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q4 0\n",
       "        Failed example:\n",
       "            isinstance(super_out, list)\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q4 0[0]>\", line 1, in <module>\n",
       "                isinstance(super_out, list)\n",
       "                           ^^^^^^^^^\n",
       "            NameError: name 'super_out' is not defined\n",
       "\n",
       "    q4 - 2 result:\n",
       "        Trying:\n",
       "            len(super_out) == 3\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q4 1\n",
       "        Failed example:\n",
       "            len(super_out) == 3\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q4 1[0]>\", line 1, in <module>\n",
       "                len(super_out) == 3\n",
       "                    ^^^^^^^^^\n",
       "            NameError: name 'super_out' is not defined\n",
       "\n",
       "    q4 - 3 result:\n",
       "        Trying:\n",
       "            all([isinstance(x, str) for x in super_out])\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q4 2\n",
       "        Failed example:\n",
       "            all([isinstance(x, str) for x in super_out])\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q4 2[0]>\", line 1, in <module>\n",
       "                all([isinstance(x, str) for x in super_out])\n",
       "                                                 ^^^^^^^^^\n",
       "            NameError: name 'super_out' is not defined\n",
       "\n",
       "q5 results:\n",
       "    q5 - 1 result:\n",
       "        Trying:\n",
       "            sum(clean_out['Skin color'].isnull()) > sum(heroes['Skin color'].isnull())\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q5 0\n",
       "        Failed example:\n",
       "            sum(clean_out['Skin color'].isnull()) > sum(heroes['Skin color'].isnull())\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q5 0[0]>\", line 1, in <module>\n",
       "                sum(clean_out['Skin color'].isnull()) > sum(heroes['Skin color'].isnull())\n",
       "                    ^^^^^^^^^\n",
       "            NameError: name 'clean_out' is not defined\n",
       "\n",
       "    q5 - 2 result:\n",
       "        Trying:\n",
       "            sum(clean_out['Weight'].isnull()) > sum(heroes['Weight'].isnull())\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q5 1\n",
       "        Failed example:\n",
       "            sum(clean_out['Weight'].isnull()) > sum(heroes['Weight'].isnull())\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q5 1[0]>\", line 1, in <module>\n",
       "                sum(clean_out['Weight'].isnull()) > sum(heroes['Weight'].isnull())\n",
       "                    ^^^^^^^^^\n",
       "            NameError: name 'clean_out' is not defined\n",
       "\n",
       "q6 results:\n",
       "    q6 - 1 result:\n",
       "        Trying:\n",
       "            isinstance(stats_out[0], str)\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q6 0\n",
       "        Failed example:\n",
       "            isinstance(stats_out[0], str)\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q6 0[0]>\", line 1, in <module>\n",
       "                isinstance(stats_out[0], str)\n",
       "                           ^^^^^^^^^\n",
       "            NameError: name 'stats_out' is not defined\n",
       "\n",
       "    q6 - 2 result:\n",
       "        Trying:\n",
       "            isinstance(stats_out[1], str)\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q6 1\n",
       "        Failed example:\n",
       "            isinstance(stats_out[1], str)\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q6 1[0]>\", line 1, in <module>\n",
       "                isinstance(stats_out[1], str)\n",
       "                           ^^^^^^^^^\n",
       "            NameError: name 'stats_out' is not defined\n",
       "\n",
       "    q6 - 3 result:\n",
       "        Trying:\n",
       "            stats_out[2] in ['good', 'bad']\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q6 2\n",
       "        Failed example:\n",
       "            stats_out[2] in ['good', 'bad']\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q6 2[0]>\", line 1, in <module>\n",
       "                stats_out[2] in ['good', 'bad']\n",
       "                ^^^^^^^^^\n",
       "            NameError: name 'stats_out' is not defined\n",
       "\n",
       "    q6 - 4 result:\n",
       "        Trying:\n",
       "            stats_out[3] in ['Marvel Comics', 'DC Comics']\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q6 3\n",
       "        Failed example:\n",
       "            stats_out[3] in ['Marvel Comics', 'DC Comics']\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q6 3[0]>\", line 1, in <module>\n",
       "                stats_out[3] in ['Marvel Comics', 'DC Comics']\n",
       "                ^^^^^^^^^\n",
       "            NameError: name 'stats_out' is not defined\n",
       "\n",
       "    q6 - 5 result:\n",
       "        Trying:\n",
       "            isinstance(stats_out[4], str)\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q6 4\n",
       "        Failed example:\n",
       "            isinstance(stats_out[4], str)\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q6 4[0]>\", line 1, in <module>\n",
       "                isinstance(stats_out[4], str)\n",
       "                           ^^^^^^^^^\n",
       "            NameError: name 'stats_out' is not defined\n",
       "\n",
       "    q6 - 6 result:\n",
       "        Trying:\n",
       "            isinstance(stats_out[5], str)\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q6 5\n",
       "        Failed example:\n",
       "            isinstance(stats_out[5], str)\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q6 5[0]>\", line 1, in <module>\n",
       "                isinstance(stats_out[5], str)\n",
       "                           ^^^^^^^^^\n",
       "            NameError: name 'stats_out' is not defined\n",
       "\n",
       "q7 results:\n",
       "    q7 - 1 result:\n",
       "        Trying:\n",
       "            cleaned.shape[0] == df.shape[0]\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q7 0\n",
       "        Failed example:\n",
       "            cleaned.shape[0] == df.shape[0]\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q7 0[0]>\", line 1, in <module>\n",
       "                cleaned.shape[0] == df.shape[0]\n",
       "                ^^^^^^^\n",
       "            NameError: name 'cleaned' is not defined\n",
       "\n",
       "    q7 - 2 result:\n",
       "        Trying:\n",
       "            cleaned['nation'].nunique() == 59\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q7 1\n",
       "        Failed example:\n",
       "            cleaned['nation'].nunique() == 59\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q7 1[0]>\", line 1, in <module>\n",
       "                cleaned['nation'].nunique() == 59\n",
       "                ^^^^^^^\n",
       "            NameError: name 'cleaned' is not defined\n",
       "\n",
       "    q7 - 3 result:\n",
       "        Trying:\n",
       "            len(info) == 4\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q7 2\n",
       "        Failed example:\n",
       "            len(info) == 4\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q7 2[0]>\", line 1, in <module>\n",
       "                len(info) == 4\n",
       "                    ^^^^\n",
       "            NameError: name 'info' is not defined\n",
       "\n",
       "    q7 - 4 result:\n",
       "        Trying:\n",
       "            all([True if isinstance(x,y) else x.dtype==np.dtype(y) for x, y in zip(info, [str, float, int, str])])\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q7 3\n",
       "        Failed example:\n",
       "            all([True if isinstance(x,y) else x.dtype==np.dtype(y) for x, y in zip(info, [str, float, int, str])])\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q7 3[0]>\", line 1, in <module>\n",
       "                all([True if isinstance(x,y) else x.dtype==np.dtype(y) for x, y in zip(info, [str, float, int, str])])\n",
       "                                                                                       ^^^^\n",
       "            NameError: name 'info' is not defined\n",
       "\n",
       "    q7 - 5 result:\n",
       "        Trying:\n",
       "            (info[1] >= 0) & (info[1] <= 1)\n",
       "        Expecting:\n",
       "            True\n",
       "        **********************************************************************\n",
       "        Line 1, in q7 4\n",
       "        Failed example:\n",
       "            (info[1] >= 0) & (info[1] <= 1)\n",
       "        Exception raised:\n",
       "            Traceback (most recent call last):\n",
       "              File \"C:\\Users\\najer\\miniforge3\\envs\\dsc80\\Lib\\doctest.py\", line 1368, in __run\n",
       "                exec(compile(example.source, filename, \"single\",\n",
       "              File \"<doctest q7 4[0]>\", line 1, in <module>\n",
       "                (info[1] >= 0) & (info[1] <= 1)\n",
       "                 ^^^^\n",
       "            NameError: name 'info' is not defined"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "otter": {
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> ans =  trick_me()\n>>> ans == 1 or ans == 2 or ans == 3\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(trick_ans, list)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(trick_ans[1], int)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> out_pop.index.tolist() == ['A', 'B', 'C', 'D']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> cols = ['num_nonnull', 'prop_nonnull', 'num_distinct', 'prop_distinct']\n>>> out_pop.columns.tolist() == cols\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> bool((out_pop['num_distinct'] <= 10).all())\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> bool((out_pop['prop_nonnull'] == 1.0).all())\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> common_out.index.tolist() == [0, 1, 2]\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> common_out.columns.tolist() == ['A_values', 'A_counts', 'B_values', 'B_counts']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> bool(common_out['A_values'].isin(range(10)).all())\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(super_out, list)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> len(super_out) == 3\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> all([isinstance(x, str) for x in super_out])\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> sum(clean_out['Skin color'].isnull()) > sum(heroes['Skin color'].isnull())\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> sum(clean_out['Weight'].isnull()) > sum(heroes['Weight'].isnull())\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> isinstance(stats_out[0], str)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(stats_out[1], str)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> stats_out[2] in ['good', 'bad']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> stats_out[3] in ['Marvel Comics', 'DC Comics']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(stats_out[4], str)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> isinstance(stats_out[5], str)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> cleaned.shape[0] == df.shape[0]\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> cleaned['nation'].nunique() == 59\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> len(info) == 4\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> all([True if isinstance(x,y) else x.dtype==np.dtype(y) for x, y in zip(info, [str, float, int, str])])\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        },
        {
         "code": ">>> (info[1] >= 0) & (info[1] <= 1)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0.5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
